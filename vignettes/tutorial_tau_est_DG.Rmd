---
title: "Network time constants via dichotomized Gaussians"
---

```{r Rmdsetup, echo=FALSE}
# Set max width of markdown code outputs
options(width = 10000)
knitr::opts_chunk$set(cache = FALSE)
```

The extent to which an individual neuron feeds back on itself in a recurrent loop can be estimated by its autocorrelation, i.e., the correlation between the neuron's membrane potential $v$ at time $t_1$ and at a later time $t_2>t_1$. The more a spike *now* increases the probability of a spike *later*, the stronger the neuron's connection back onto itself. A neuron's autocorrelation, represented by the variable $R$, can be modeled with an exponential decay function \[R = A\exp(-l/\tau) + b\] where $A$ is the *amplitude* (autocorrelation at the initial lag), $l$ is lag, $\tau$ is the *network time constant*, and $b$ is a constant (bias or baseline) term. The time constant $\tau$ is a measure of how quickly the neuron's autocorrelation decays back to baseline after a spike.

Network time constants are difficult to estimate from experimental data. The spiking activity of a neuron is indicative of its recurrence only if that neuron is receiving no other input. Thus, time constants must be estimated from periods of spontaneous activity. These periods are short and noisy, making time constant estimates from empirical calculations of a neuron's autocorrelation unreliable. A way to improve the signal-to-noise ratio is needed, such as simulating many recordings. However, typical approaches to such simulations, such as bootstrapping, will only amplify the noise. A better approach is to use dichotomized Gaussians. 

This tutorial shows how to use the neurons package to estimate network time constants using dichotomized Gaussians. Patch-clamp recordings will be used as an example dataset. The recordings are from layer 2/3 of the auditory cortex of mature wildtype mice, in both the left and right hemisphere. These recordings are used by [Neophytou et al. 2022](https://doi.org/10.1371/journal.pbio.3001803), who adapt and apply the dichotomized Gaussian approach of [Macke et al. 2009](https://doi.org/10.1162/neco.2008.02-08-713) to show that the right auditory cortex of mice has more recurrence than the left. This tutorial reproduces that analysis, with a few improvements. 

<h2>Load spike rasters</h2>

Set up the R environment by clearing the workspace, setting a random-number generator seed, and loading the neurons package.

```{r Rsetup}
# Clear the R workspace to start fresh
rm(list = ls())

# Set seed for reproducibility
set.seed(12345) 

# Load neurons package
library(neurons) 
```

All of the data is contained in a single csv file, provided with the neurons package, as a compact spike raster. 

```{r load rasters}
spike.rasters <- read.csv(
  system.file(
      "extdata", 
      "spike_rasters_2022data.csv", 
      package = "neurons"
    )
  )
print(head(spike.rasters))
```

The data takes the form of a dataframe the rows of which each represent a single recorded spike. Each column gives relevant metadata, such as the time in the recording of the spike, the identity of the neuron that fired the spike, and the hemisphere in which that neuron was recorded. The function [load.rasters.as.neurons](../reference/load.rasters.as.neurons.html) will convert a compact raster of spikes like this one (a dataframe or file name to a csv importable as such) into **neuron** objects (one per cell), so long as it has the recognized columns: **cell**, **time_in_ms**, and **trial**. If the optional columns **recording_name**, **hemisphere**, **genotype**, **sex**, **region**, or **age** are included, they will be recognized and added as metadata to the neuron objects. 

```{r create_neuron_objects}
neurons <- load.rasters.as.neurons(spike.rasters, bin_size = 5)
cat("Number of cells discovered:", length(neurons))
```

The **neuron** object class is native to C++ and integrated into neurons (an R package) via Rcpp. It comes with built-in methods for many tasks, such as plotting rasters, plotting autocorrelation, and estimating autocorrelation parameters with dichotomized Gaussian simulations. Some of these methods can be accessed through R, but neurons provides R-native wrappers for the most useful ones. The neurons package also provides native R functions for plotting. Let's plot the rasters for two cells. The first has high autocorrelation, as can be seen from the long horizontal streaks of spikes:

```{r plot_raster_high_autocor}
cell_high <- 16
plot.raster(neurons[[cell_high]]) 
```

The second has low autocorrelation, as can be seen from the more random distribution of spikes without long streaks:  

```{r plot_raster_low_autocor}
cell_low <- 1
plot.raster(neurons[[cell_low]]) 
```

<h2>Computing empirical autocorrelation from data</h2>

There are two common definitions for the correlation $R_{XY}$ between two random variables $X$ and $Y$. Let $\text{E}[X]$, $\text{E}[Y]$, and $\text{E}[XY]$ be the expected values of $X$, $Y$, and their product $XY$. The first definition, the *raw correlation*, is: 
\[R_{XY} = \text{E}[XY]\]
The second, the *Pearson correlation*, centers and normalizes the raw correlation: 
\[R_{XY} = \frac{\text{E}[XY] - \text{E}[X]\text{E}[Y]}{\sqrt{\text{E}[X^2] - \text{E}[X]^2}\sqrt{\text{E}[Y^2] - \text{E}[Y]^2}}\]
While only the Pearson correlation will return values constrained to the range $[-1,1]$, the raw correlation is well-defined for a broader range of cases, including cases where empirical estimates need to be made from observations with little to no variance, such as spike rasters with low firing rates.

The above equations are for theoretical *population* values, given in terms of the expected value operator. Both definitions have empirical analogues which can be used to calculate a value directly from a finite sample. If $\vec{X}=\langle X_1,\ldots,X_T\rangle$ and $\vec{Y}=\langle Y_1,\ldots,Y_T\rangle$ are each a series of observations collected from $X$ and $Y$, then the empirical raw correlation is given in terms of the dot product: 
\[R_{XY} = \frac{\vec{X}\cdot\vec{Y}}{T} = \frac{1}{T}\sum_{i=1}^T X_i Y_i\]
If $\mu_{\vec{XY}}$ is the empirical mean of the vector $\vec{XY}$ of products $X_iY_i$, the raw correlation can also be computed as $R_{XY} = \mu_{\vec{XY}}$. The empirical Pearson correlation between $\vec{X}$ and $\vec{Y}$ with means $\mu_\vec{X}$ and $\mu_\vec{Y}$ and standard deviations $\sigma_\vec{X}$ and $\sigma_\vec{Y}$ is given by:
\[R_{XY} = \frac{\mu_{\vec{XY}} - \mu_\vec{X}\mu_\vec{Y}}{\sigma_\vec{X} \sigma_\vec{Y}}\]
For ease of reading, the vector notation will hereafter be dropped and capital letters will be used to refer to both the random variables and their empirical samples, with the understanding that the context will make clear which is meant.

<h3>Time series and repeated observations</h3>

If $X$ is a time series $X=\langle{}X_1, \ldots, X_t, \ldots, X_T\rangle{}$, then its autocorrelation at lag $l$, denoted $R_{XX}(l)$, is the correlation $R_{XY}$ between $X$ and a copy $Y=\langle{}X_{l+1}, X_{l+2}, X_{l+3}, \ldots \rangle{}$ of $X$ time shifted by some lag $l$. This lagged copy will be shorter than $X$ by $l$ samples, so computing the empirical autocorrelation requires adjusting the summation index and the normalization term accordingly. For example, the empirical raw autocorrelation at lag $l$ is given by:
\[R_{XX}(l) = \frac{1}{T-l}\sum_{i=1}^{T-l} X_i Y_{i+l}\]
If $X$ and $Y$ are matrices containing many trials (columns) of sample series (rows) of data collected from the random variables $X$ and $Y$, then the empirical correlation between $X$ and $Y$ can be refined by averaging together the correlations computed for each trial. For example, the empirical raw autocorrelation at lag $l$ is given by:
\[R_{XX}(l) = \frac{1}{N}\sum_{j=1}^N \frac{1}{T-l}\sum_{i=1}^{T-l} X_{ij} Y_{(i+l)j}\]
where $N$ is the number of trials (columns) in $X$ and $Y$.

<h3>Time binning</h3>

The **neuron** object class has built-in methods for computing both the raw and Pearson empirical autocorrelation from a spike raster $S$, which is (of course) just a matrix for a random variable (spike or no-spike) sampled over time (rows) and trials (columns). The variable $S$ is discrete, with only two possible values: 1 for a spike, 0 for no spike. This binary nature means that $S$ does not play nicely with any of the empirical formulas given above. If given the raw binary input, these formulas will return zero or near zero autocorrelation. 

For more cogent results, the rows (i.e., the time axis) of the matrix $S$ must be downsampled via binning. This binning is done automatically by the function [load.rasters.as.neurons](../reference/load.rasters.as.neurons.html) and can be controlled via its argument **bin_size**, which below is represented by $\Delta$. The default, used in the code above, is 5ms. 

A further question to decide when computing empirical autocorrelation is how to handle multiple spikes in a single bin. There are three options supported by the neurons package: "sum", "mean", and "boolean". The option is set via the argument **bin_count_action**, available in the functions [compute.autocorr](../reference/compute.autocorr.html), [process.autocorr](../reference/process.autocorr.html), and [estimate.autocorr.params](../reference/estimate.autocorr.params.html). In all cases, the default is "sum", meaning that the value of $S$ in a given bin on a given trial is the total count of spikes falling in that bin. The option "mean" will instead return the mean number of spikes in the bin, while "boolean" will return 1 if there is at least one spike in the bin and 0 otherwise. From *ad hoc* development and testing, the "sum" function works best, presumably because it preserves the most information about correlation. 

Time binning introduces a dilemma. On the one hand, it's essential for avoiding autocorrelation values of zero. On the other hand, it violates the mathematical assumptions of the dichotomized Gaussian approach. The violation will be pointed out below, but for now note that the best approach seems to be a middle path. A large time bin, such as 20ms, results in robust autocorrelation estimates from the spike data, but leads to unreliable dichotomized Gaussian simulations. A small time bin, such as 1ms, leads to unreliable empirical autocorrelation estimates, but reliable dichotomized Gaussian simulations. The default (5ms) falls in the middle. Whatever the chosen bin size, a rolling mean is taken to smooth the empirical autocorrelation value. 

<h3>Example autocorrelation calculations</h3>

The function [compute.autocorr](../reference/compute.autocorr.html) takes a single neuron and computes its empirical autocorrelation using its spike raster. Here, for example, is the function used to compute the raw autocorrelation for the neuron with high autocorrelation shown above:

```{r compute_autocorrelation_high}
compute.autocorr(neurons[[cell_high]], use_raw = TRUE)
plot.autocorrelation(neurons[[cell_high]])
```

Which type of correlation, raw or Peason, is calculated is controlled by the **use_raw** option. The Pearson autocorrelation for the same data can be got by setting **use_raw** to FALSE: 

```{r compute_autocorrelation_high_pearson}
compute.autocorr(neurons[[cell_high]], use_raw = FALSE)
plot.autocorrelation(neurons[[cell_high]])
```

As another example, here is the raw autocorrelation for the neuron with low autocorrelation shown above:

```{r compute_autocorrelation_low}
compute.autocorr(neurons[[cell_low]], use_raw = TRUE)
plot.autocorrelation(neurons[[cell_low]])
```

And the Pearson autocorrelation for the same data:

```{r compute_autocorrelation_low_pearson}
compute.autocorr(neurons[[cell_low]], use_raw = FALSE)
plot.autocorrelation(neurons[[cell_low]])
```

<h2>Modeling autocorrelation decay</h2>

Theoretically, autocorrelation can be expected to exhibit exponential decay over increasing lag, at least in cases with nonzero autocorrelation. As noted above, this decay can be modeled with the function: 
\[R = A\exp(-l/\tau) + b\]
where $A$ is the *amplitude* (autocorrelation at the initial lag), $l$ is lag, $\tau$ is the *network time constant*, and $b$ is a constant (bias or baseline) term. The neurons package assumes that the bias term $b$ is a constant function of the firing rate $\lambda$ and bin size $\Delta$, given as: 
\[b = (\lambda{}\Delta)^2\] 
In this formula both $\lambda$ and $\Delta$ must be in the same unit of time, e.g., ms. The values for $A$ and $\tau$ are set by minimizing the mean squared error between the empirical autocorrelation and the model function, using the [L-BFGS algorithm as implemented in NLopt](https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#low-storage-bfgs). 

Model fitting is accessed in the neurons package with the function [fit.edf.autocorr](../reference/fit.edf.autocorr.html). The function takes a single neuron and fits the exponential decay function to its empirical autocorrelation. Here, for example, is the function used to fit the model to the raw autocorrelation for the neuron with high autocorrelation:

```{r fit_autocorrelation_high}
compute.autocorr(neurons[[cell_high]]) # Recompute with raw autocorrelation
fit.edf.autocorr(neurons[[cell_high]])
plot.autocorrelation(neurons[[cell_high]])
```

The fitted model is plotted as the red line. The parameters of the exponential decay fit can be fetched directly with a neuron method and provide succinct quantification of the empirical autocorrelation. 

```{r fetch_fit_parameters_high}
print(neurons[[cell_high]]$fetch_EDF_parameters())
```

In this case, the time constant tau is estimated to be 33ms and the initial autocorrelation $A$ is estimated to be 0.04. 

The above shows empirical autocorrelation and model fit being performed in separate steps and for only one neuron at a time. The function [process.autocorr](../reference/process.autocorr.html) will perform both steps at once for an entire list of neurons and return the results in a data frame. 

```{r process_autocorr_as_batch}
autocor.results.batch <- process.autocorr(neurons)
print(head(autocor.results.batch))
```

<h2>Estimating network time constants</h2>

The above discussion concerns computing and modeling *empirical* autocorrelation, i.e., autocorrelation as computed directly off a finite sample. However, what's usually desired is an estimate of the *population* value, i.e., the true autocorrelation exhibited by a population of cells defined by some shared covariate value. Estimating this true value is done by taking an infinite sample, sampling not just all existing population members, but also all possible members. This is, of course, impossible. However, it can be approximated by taking larger and larger samples. The ideal, of course, would be to take samples of the actual population, e.g., recording more cells, or recording more trials from the same cells. However, in practice this is not possible. Instead, mathematical techniques are used to simulate larger samples from existing data. The most well-known technique is bootstrapping, i.e., "resampling" the observed data with replacement. Bootstrapping does a good job perserving the underlying statistical distribution of data when the signal-to-noise ratio is high, but when the signal-to-noise ratio is low, bootstrapping will simply amplify the noise. This is the case with autocorrelation estimated from spike rasters, especially when the firing rate is low and the recording time is short.

<h3>Dichotomized Gaussians</h3>

Instead of resampling with replacement, an alternative approach is to simulate new samples through a random-process model that's constrained to be consistent with the observed data. In the case of exponentially decaying autocorrelation, dichotomized Gaussians provide an ideal model. The basic idea is to model the noisy processes underlying neuron spiking across time as a latent multivariate Gaussian process, with one Gaussian distribution per time bin. On this model, autocorrelation is modeled as correlation between these Gaussians. 

Consider the following example of a dichotomized Gaussian. First, let's draw a random sample of 300 points from a bivariate Gaussian distribution $V$, such that both component distributions $V_1$ and $V_2$ are normal (i.e., have mean $\mu$ of 0 and standard deviation $\sigma$ of 1) and such that a covariance $K_V$ of 0.75 exists between these distributions.

```{r DG_plot_data}
V_sample <- MASS::mvrnorm(
    n = 300,
    mu = c(0,0),
    Sigma = matrix( 
        c(1, 0.75, 
          0.75, 1), 
        nrow = 2, 
        ncol = 2
      ) 
  )
```

Next, let's plot these points and superimpose on top of them thresholds $\gamma = 1$ for both dimensions, shading the area of points below the threshold. 

```{r DG_plot}
# Convert to data frame for plotting
V_sample <- as.data.frame(V_sample) 
threshold <- 1
# Make and print plot
ggplot2::ggplot(data = V_sample, ggplot2::aes(x = V_sample[,1], y = V_sample[,2])) +
  ggplot2::geom_point() +
  ggplot2::labs(
    x = "V1", 
    y = "V2", 
    title = "Example bivariate data") +
  ggplot2::ylim(c(-4.5,4.5)) +
  ggplot2::xlim(c(-4.5,4.5)) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    panel.background = ggplot2::element_rect(fill = "white", colour = NA),
    plot.background  = ggplot2::element_rect(fill = "white", colour = NA)) +
  ggplot2::geom_vline(xintercept = 0, color = "darkgray", linewidth = 1) +
  ggplot2::geom_hline(yintercept = 0, color = "darkgray", linewidth = 1) +
  ggplot2::geom_vline(xintercept = threshold, color = "darkblue", linewidth = 1) +
  ggplot2::geom_hline(yintercept = threshold, color = "darkblue", linewidth = 1) +
  ggplot2::annotate(
    "rect", 
    xmin = -Inf, xmax = threshold, 
    ymin = -Inf, ymax = threshold, 
    fill = "lightblue", alpha = 0.3) +
  ggplot2::annotate(
    "text", x = threshold + 0.35, y = 4.5, 
    label = "gamma", parse = TRUE, color = "darkblue", 
    size = 7, hjust = 0) +
  ggplot2::annotate(
    "text", x = 4.5, y = threshold + 0.35, 
    label = "gamma", parse = TRUE, color = "darkblue", 
    size = 7, vjust = 0)
```

Notice how a threshold value $\gamma$ defines, for any $V_i$, a new binary random variable $X$ such that $X=1$ if $V_i>\gamma$ and $X=0$ otherwise. The variable $X$ is the "dichotomized Gaussian". 

<h3>Simulating spike rate</h3>

If $X$ is a dichotomized Gaussian, the probability $P(X=1)$ that $X$ is 1 is given by the cumulative distribution function of $V_i$ evaluated at $\gamma$. As each $V_i$ is stipulated to be a standard normal ($\mu=0$, $\sigma=1$), this cumulative distribution function is the standard normal cumulative distribution function $\Phi$:
\[\Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-t^2/2} dt\]
Thus:
\[P(X=1) = P(V_i>\gamma) = 1 - \Phi(\gamma)\]
In the above plot, the function $\Phi(\gamma)$ corresponds to the shaded area of each axis, while $1 - \Phi(\gamma)=P(X=1)$ corresponds to the non-shaded area.

It follows that $V_i$ can be used to simulate a neuron with mean spike rate $\lambda$ by setting the threshold $\gamma$ such that: 
\[\lambda = 1 - \Phi(\gamma)\]
which means that: 
\[\gamma = \Phi^{-1}(1-\lambda)\]
where $\Phi^{-1}$ is the inverse of $\Phi$, i.e., is the quantile function. This quantile function can be computed via well-known numerical approximations, meaning that a dichotomized Gaussian can easily be used to simulate a neuron with a desired mean spike rate $\lambda$.

<h3>Simulating autocorrelation</h3>

While simulating a given spike rate $\lambda$ is straightforward, simulating autocorrelation is not. How is autocorrelation to be represented in the model? If the correlation between two component dimensions $V_{i_1}$ and $V_{i_2}$ of a multivariate Gaussian is to represent the autocorrelation between two time bins $X_{i_1}$ and $X_{i_2}$ of a spike raster separated by lag $l$, then each $V_i$ should not be thought of as a separate neuron, but rather as the same neuron at different time points. 

While this is an insightful idea, it's mathematically impossible. The issue is that the operation of thresholding, needed to convert a Gaussian variable $V_i$ into a simulated binary spike variable $X_i$, will change the correlation: 
\[R_{V_{i_1}V_{i_2}} \neq R_{X_{i_1}X_{i_2}}\]
However, just as there is a predictable relationship between the threshold $\gamma$ and spike rate $\lambda$, there is also a predictable relationship between the correlations $R_{V_{i_1}V_{i_2}}$ and $R_{X_{i_1}X_{i_2}}$. As there are two types of correlation at issue, raw and Pearson, we will frame the result in terms of covariance $K$.

Suppose we know the autocorrelation $R_{X_{i_1}X_{i_2}}$ between two time bins $X_{i_1}$ and $X_{i_2}$ separated by lag $l$ and know that the neuron $X$ spikes with rate $\lambda$ across all time bins. We want to find the covariance $K_{V_{i_1}V_{i_2}}$ between the corresponding Gaussian variables $V_{i_1}$ and $V_{i_2}$ which, when these variables are thresholded by $\gamma = \Phi^{-1}(1-\lambda)$, gives back the correlation $R_{X_{i_1}X_{i_2}}$. 

Let $\Phi_2^+(x,K_V)$ be the upper-tail cumulative distribution function of a bivariate normal distribution with covariance $K_V$, evaluated at $x$ for both components: 
\[\Phi_2^+(x,K_V) = P(V_1>x, V_2>x \;|\; V_1, V_2\sim \text{MVN}(\mu=0,\sigma=1,K=K_V))\frac{1}{\sqrt{2\pi}} \int_{x}^\infty\int_{x}^\infty e^{-t^2/2} dt\]
That is, $\Phi_2(x,K_V)$ is the probability that all components of a multivariate normal distribution with covariance $K_V$ are less than or equal to $x$.

By definition, the empirical raw autocorrelation $R_{X_{i_1}X_{i_2}}$ can be written in terms of autocovariance, represented by the variable $K$, as follows: 
\[R_{X_{i_1}X_{i_2}} = K_{X_{i_1}X_{i_2}} + \mu_{X_{i_1}}\mu_{X_{i_2}} = \text{Cov}(X_{i_1}, X_{i_2}) + \lambda^2\]

```{r}

# Run simulations
autocor.ests <- estimate.autocorr.params(
  neuron_list = neurons,
  n_trials_per_sim = 500, 
  n_sims_per_neurons = 100,
  use_raw = TRUE
  )

# Run analysis
autocor.results.bootstraps <- analyze.autocorr(
  autocor.ests,
  covariate = c("hemi","genotype"),
  n_bs = 1e4
)
print(autocor.results.bootstraps$distribution_plot)

```
