<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Network time constants via dichotomized Gaussians • neurons</title>
<script src="../lightswitch.js"></script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="Network time constants via dichotomized Gaussians">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top " aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">neurons</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../index.html">Introduction</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Package Functions</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Installation</h6></li>
    <li><a class="dropdown-item" href="../articles/install.html">Installation instructions</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Tutorials</h6></li>
    <li><a class="dropdown-item" href="../articles/tutorial_synchrony_crosscorr.html">Spike synchrony via cross-correlation</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial_tau_est_DG.html">Network time constants via dichotomized Gaussians</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial_tau_est_kilosort.html">Network time constants from KiloSort4 data</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Legal</h6></li>
    <li><a class="dropdown-item" href="../articles/license.html">Dependency licenses</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/michaelbarkasi/neurons"><span class="fa fab fa-github"></span></a></li>
<li class="nav-item"><a class="nav-link" href="../#">🌙</a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Network time constants via dichotomized Gaussians</h1>
            
      

      <div class="d-none name"><code>tutorial_tau_est_DG.Rmd</code></div>
    </div>

    
    
<p>The extent to which an individual neuron feeds back on itself in a
recurrent loop can be estimated by its autocorrelation, i.e., the
correlation between the neuron’s membrane potential
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>
at time
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding="application/x-tex">t_1</annotation></semantics></math>
and at a later time
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mn>2</mn></msub><mo>&gt;</mo><msub><mi>t</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">t_2&gt;t_1</annotation></semantics></math>.
The more a spike <em>now</em> increases the probability of a spike
<em>later</em>, the stronger the neuron’s connection back onto itself. A
neuron’s autocorrelation, represented by the variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>,
can be modeled with an exponential decay function
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>=</mo><mi>A</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mi>l</mi><mi>/</mi><mi>τ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">R = A\exp(-l/\tau) + b</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
is the <em>amplitude</em> (autocorrelation at the initial lag),
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
is lag,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>
is the <em>network time constant</em>, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>
is a constant (bias or baseline) term. The time constant
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>
is a measure of how quickly the neuron’s autocorrelation decays back to
baseline after a spike.</p>
<p>Network time constants are difficult to estimate from experimental
data. The spiking activity of a neuron is indicative of its recurrence
only if that neuron is receiving no other input. Thus, time constants
must be estimated from periods of spontaneous activity. These periods
are short and noisy, making time constant estimates from empirical
calculations of a neuron’s autocorrelation unreliable. A way to improve
the signal-to-noise ratio is needed, such as simulating many recordings.
However, typical approaches to such simulations, such as bootstrapping,
will only amplify the noise. A better approach is to use dichotomized
Gaussians.</p>
<p>This tutorial shows how to use the neurons package to estimate
network time constants using dichotomized Gaussians. Patch-clamp
recordings will be used as an example dataset. The recordings are from
layer 2/3 of the auditory cortex of mature wildtype mice, in both the
left and right hemisphere. These recordings are used by <a href="https://doi.org/10.1371/journal.pbio.3001803" class="external-link">Neophytou et
al. 2022</a>, who adapt and apply the dichotomized Gaussian approach of
<a href="https://doi.org/10.1162/neco.2008.02-08-713" class="external-link">Macke et
al. 2009</a> to show that the right auditory cortex of mice has more
recurrence than the left. This tutorial reproduces that analysis, with a
few improvements.</p>
<h2>
Load spike rasters
</h2>
<p>Set up the R environment by clearing the workspace, setting a
random-number generator seed, and loading the neurons package.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Clear the R workspace to start fresh</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/rm.html" class="external-link">rm</a></span><span class="op">(</span>list <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ls.html" class="external-link">ls</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Set seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">12345</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Load neurons package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">)</span> </span></code></pre></div>
<p>All of the data is contained in a single csv file, provided with the
neurons package, as a compact spike raster.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">spike.rasters</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html" class="external-link">read.csv</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span></span>
<span>      <span class="st">"extdata"</span>, </span>
<span>      <span class="st">"spike_rasters_2022data.csv"</span>, </span>
<span>      package <span class="op">=</span> <span class="st">"neurons"</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">spike.rasters</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##   trial sample cell time_in_ms recording_name hemi genotype sex    age region</span></span>
<span><span class="co">## 1     2   1181    1      118.1    20080930002   LH    CBA/J   M P30:60    ACx</span></span>
<span><span class="co">## 2     3   1286    1      128.6    20080930002   LH    CBA/J   M P30:60    ACx</span></span>
<span><span class="co">## 3     3  13537    1     1353.7    20080930002   LH    CBA/J   M P30:60    ACx</span></span>
<span><span class="co">## 4     4    691    1       69.1    20080930002   LH    CBA/J   M P30:60    ACx</span></span>
<span><span class="co">## 5     4   2404    1      240.4    20080930002   LH    CBA/J   M P30:60    ACx</span></span>
<span><span class="co">## 6     4   3746    1      374.6    20080930002   LH    CBA/J   M P30:60    ACx</span></span></code></pre>
<p>The data takes the form of a dataframe the rows of which each
represent a single recorded spike. Each column gives relevant metadata,
such as the time in the recording of the spike, the identity of the
neuron that fired the spike, and the hemisphere in which that neuron was
recorded. The function <a href="../reference/load.rasters.as.neurons.html">load.rasters.as.neurons</a>
will convert a compact raster of spikes like this one (a dataframe or
file name to a csv importable as such) into <strong>neuron</strong>
objects (one per cell), so long as it has the recognized columns:
<strong>cell</strong>, <strong>time_in_ms</strong>, and
<strong>trial</strong>. If the optional columns
<strong>recording_name</strong>, <strong>hemisphere</strong>,
<strong>genotype</strong>, <strong>sex</strong>,
<strong>region</strong>, or <strong>age</strong> are included, they will
be recognized and added as metadata to the neuron objects.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">neurons</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load.rasters.as.neurons.html">load.rasters.as.neurons</a></span><span class="op">(</span><span class="va">spike.rasters</span>, bin_size <span class="op">=</span> <span class="fl">10.0</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Number of cells discovered:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Number of cells discovered: 41</span></span></code></pre>
<p>The <strong>neuron</strong> object class is native to C++ and
integrated into neurons (an R package) via Rcpp. It comes with built-in
methods for many tasks, such as estimating autocorrelation parameters
with dichotomized Gaussian simulations. Some of these methods can be
accessed through R, but neurons provides R-native wrappers for the most
useful ones. The neurons package also provides native R functions for
plotting. Let’s plot the rasters for two cells. The first has high
autocorrelation, as can be seen from the long horizontal streaks of
spikes:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cell_high</span> <span class="op">&lt;-</span> <span class="fl">16</span></span>
<span><span class="fu"><a href="../reference/plot-raster.html">plot.raster</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span><span class="op">)</span> </span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/plot_raster_high_autocor-1.png" width="700"></p>
<p>The second has low autocorrelation, as can be seen from the more
random distribution of spikes without long streaks:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cell_low</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="fu"><a href="../reference/plot-raster.html">plot.raster</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_low</span><span class="op">]</span><span class="op">]</span><span class="op">)</span> </span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/plot_raster_low_autocor-1.png" width="700"></p>
<h2>
Computing empirical autocorrelation from data
</h2>
<p>There are two common definitions for the correlation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><annotation encoding="application/x-tex">R_{XY}</annotation></semantics></math>
between two random variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>.
Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\text{E}[X]</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\text{E}[Y]</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mi>Y</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\text{E}[XY]</annotation></semantics></math>
be the expected values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>,
and their product
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>Y</mi></mrow><annotation encoding="application/x-tex">XY</annotation></semantics></math>.
The first definition, the <em>raw correlation</em>, is:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo>=</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mi>Y</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">R_{XY} = \text{E}[XY]</annotation></semantics></math>
The second, the <em>Pearson correlation</em>, centers and normalizes the
raw correlation:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo>=</mo><mfrac><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mi>Y</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>−</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mo stretchy="true" form="postfix">]</mo></mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><mrow><msqrt><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>X</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">]</mo></mrow><mo>−</mo><mtext mathvariant="normal">E</mtext><msup><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mo stretchy="true" form="postfix">]</mo></mrow><mn>2</mn></msup></mrow></msqrt><msqrt><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>Y</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">]</mo></mrow><mo>−</mo><mtext mathvariant="normal">E</mtext><msup><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mo stretchy="true" form="postfix">]</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mfrac></mrow><annotation encoding="application/x-tex">R_{XY} = \frac{\text{E}[XY] - \text{E}[X]\text{E}[Y]}{\sqrt{\text{E}[X^2] - \text{E}[X]^2}\sqrt{\text{E}[Y^2] - \text{E}[Y]^2}}</annotation></semantics></math>
While only the Pearson correlation will return values constrained to the
range
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>−</mi><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[-1,1]</annotation></semantics></math>,
the raw correlation is well-defined for a broader range of cases,
including cases where empirical estimates need to be made from
observations with little to no variance, such as spike rasters with low
firing rates.</p>
<p>The above equations are for theoretical <em>population</em> values,
given in terms of the expected value operator. Both definitions have
empirical analogues which can be used to calculate a value directly from
a finite sample. If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>X</mi><mo accent="true">→</mo></mover><mo>=</mo><mo stretchy="false" form="prefix">⟨</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>X</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">⟩</mo></mrow><annotation encoding="application/x-tex">\vec{X}=\langle X_1,\ldots,X_T\rangle</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Y</mi><mo accent="true">→</mo></mover><mo>=</mo><mo stretchy="false" form="prefix">⟨</mo><msub><mi>Y</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>Y</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">⟩</mo></mrow><annotation encoding="application/x-tex">\vec{Y}=\langle Y_1,\ldots,Y_T\rangle</annotation></semantics></math>
are each a series of observations collected from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>,
then the empirical raw correlation is given in terms of the dot product:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo>=</mo><mfrac><mrow><mover><mi>X</mi><mo accent="true">→</mo></mover><mo>⋅</mo><mover><mi>Y</mi><mo accent="true">→</mo></mover></mrow><mi>T</mi></mfrac><mo>=</mo><mfrac><mn>1</mn><mi>T</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><msub><mi>X</mi><mi>i</mi></msub><msub><mi>Y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">R_{XY} = \frac{\vec{X}\cdot\vec{Y}}{T} = \frac{1}{T}\sum_{i=1}^T X_i Y_i</annotation></semantics></math>
If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>μ</mi><mover><mrow><mi>X</mi><mi>Y</mi></mrow><mo accent="true">→</mo></mover></msub><annotation encoding="application/x-tex">\mu_{\vec{XY}}</annotation></semantics></math>
is the empirical mean of the vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mrow><mi>X</mi><mi>Y</mi></mrow><mo accent="true">→</mo></mover><annotation encoding="application/x-tex">\vec{XY}</annotation></semantics></math>
of products
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><msub><mi>Y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">X_iY_i</annotation></semantics></math>,
the raw correlation can also be computed as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo>=</mo><msub><mi>μ</mi><mover><mrow><mi>X</mi><mi>Y</mi></mrow><mo accent="true">→</mo></mover></msub></mrow><annotation encoding="application/x-tex">R_{XY} = \mu_{\vec{XY}}</annotation></semantics></math>.
The empirical Pearson correlation between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>X</mi><mo accent="true">→</mo></mover><annotation encoding="application/x-tex">\vec{X}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>Y</mi><mo accent="true">→</mo></mover><annotation encoding="application/x-tex">\vec{Y}</annotation></semantics></math>
with means
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>μ</mi><mover><mi>X</mi><mo accent="true">→</mo></mover></msub><annotation encoding="application/x-tex">\mu_\vec{X}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>μ</mi><mover><mi>Y</mi><mo accent="true">→</mo></mover></msub><annotation encoding="application/x-tex">\mu_\vec{Y}</annotation></semantics></math>
and standard deviations
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>σ</mi><mover><mi>X</mi><mo accent="true">→</mo></mover></msub><annotation encoding="application/x-tex">\sigma_\vec{X}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>σ</mi><mover><mi>Y</mi><mo accent="true">→</mo></mover></msub><annotation encoding="application/x-tex">\sigma_\vec{Y}</annotation></semantics></math>
is given by:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo>=</mo><mfrac><mrow><msub><mi>μ</mi><mover><mrow><mi>X</mi><mi>Y</mi></mrow><mo accent="true">→</mo></mover></msub><mo>−</mo><msub><mi>μ</mi><mover><mi>X</mi><mo accent="true">→</mo></mover></msub><msub><mi>μ</mi><mover><mi>Y</mi><mo accent="true">→</mo></mover></msub></mrow><mrow><msub><mi>σ</mi><mover><mi>X</mi><mo accent="true">→</mo></mover></msub><msub><mi>σ</mi><mover><mi>Y</mi><mo accent="true">→</mo></mover></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">R_{XY} = \frac{\mu_{\vec{XY}} - \mu_\vec{X}\mu_\vec{Y}}{\sigma_\vec{X} \sigma_\vec{Y}}</annotation></semantics></math>
For ease of reading, the vector notation will hereafter be dropped and
capital letters will be used to refer to both the random variables and
their empirical samples, with the understanding that the context will
make clear which is meant.</p>
<h3>
Time series and repeated observations
</h3>
<p>If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
is a time series
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mo stretchy="false" form="prefix">⟨</mo><mrow></mrow><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>X</mi><mi>t</mi></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>X</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">⟩</mo><mrow></mrow></mrow><annotation encoding="application/x-tex">X=\langle{}X_1, \ldots, X_t, \ldots, X_T\rangle{}</annotation></semantics></math>,
then its autocorrelation at lag
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>,
denoted
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>X</mi><mi>X</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">R_{XX}(l)</annotation></semantics></math>,
is the correlation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><annotation encoding="application/x-tex">R_{XY}</annotation></semantics></math>
between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and a copy
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy="false" form="prefix">⟨</mo><mrow></mrow><msub><mi>X</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>X</mi><mrow><mi>l</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>,</mo><msub><mi>X</mi><mrow><mi>l</mi><mo>+</mo><mn>3</mn></mrow></msub><mo>,</mo><mi>…</mi><mo stretchy="false" form="postfix">⟩</mo><mrow></mrow></mrow><annotation encoding="application/x-tex">Y=\langle{}X_{l+1}, X_{l+2}, X_{l+3}, \ldots \rangle{}</annotation></semantics></math>
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
time shifted by some lag
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>.
This lagged copy will be shorter than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
samples, so computing the empirical autocorrelation requires adjusting
the summation index and the normalization term accordingly. For example,
the empirical raw autocorrelation at lag
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
is given by:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>X</mi><mi>X</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mi>l</mi></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mi>l</mi></mrow></munderover><msub><mi>X</mi><mi>i</mi></msub><msub><mi>Y</mi><mrow><mi>i</mi><mo>+</mo><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">R_{XX}(l) = \frac{1}{T-l}\sum_{i=1}^{T-l} X_i Y_{i+l}</annotation></semantics></math>
If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
are matrices containing many trials (columns) of sample series (rows) of
data collected from the random variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>,
then the empirical correlation between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
can be refined by averaging together the correlations computed for each
trial. For example, the empirical raw autocorrelation at lag
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
is given by:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>X</mi><mi>X</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mi>l</mi></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mi>l</mi></mrow></munderover><msub><mi>X</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>Y</mi><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo>+</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">R_{XX}(l) = \frac{1}{N}\sum_{j=1}^N \frac{1}{T-l}\sum_{i=1}^{T-l} X_{ij} Y_{(i+l)j}</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
is the number of trials (columns) in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>.</p>
<h3>
Time binning
</h3>
<p>The <strong>neuron</strong> object class has built-in methods for
computing both the raw and Pearson empirical autocorrelation from a
spike raster
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>,
which is (of course) just a matrix for a random variable (spike or
no-spike) sampled over time (rows) and trials (columns). The variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
is discrete, with only two possible values: 1 for a spike, 0 for no
spike. This binary nature means that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
does not play nicely with any of the empirical formulas given above. If
given the raw binary input, these formulas will return zero or near zero
autocorrelation.</p>
<p>For more cogent results, the rows (i.e., the time axis) of the matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
must be downsampled via binning. This binning is done automatically by
the function <a href="../reference/load.rasters.as.neurons.html">load.rasters.as.neurons</a>
and can be controlled via its argument <strong>bin_size</strong>, which
below is represented by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Δ</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math>.
The default, used in the code above, is 10ms.</p>
<p>A further question to decide when computing empirical autocorrelation
is how to handle multiple spikes in a single bin. There are three
options supported by the neurons package: “sum”, “mean”, and “boolean”.
The option is set via the argument <strong>bin_count_action</strong>,
available in the functions <a href="../reference/compute.autocorr.html">compute.autocorr</a>, <a href="../reference/process.autocorr.html">process.autocorr</a>, and <a href="../reference/estimate.autocorr.params.html">estimate.autocorr.params</a>.
In all cases, the default is “sum”, meaning that the value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
in a given bin on a given trial is the total count of spikes falling in
that bin. The option “mean” will instead return the mean number of
spikes in the bin, while “boolean” will return 1 if there is at least
one spike in the bin and 0 otherwise. From <em>ad hoc</em> development
and testing, the “sum” function works best, presumably because it
preserves the most information about correlation.</p>
<p>Time binning introduces a dilemma. On the one hand, it’s essential
for avoiding autocorrelation values of zero. On the other hand, it
violates a key mathematical fact underlying the dichotomized Gaussian
approach. This is the fact, explained below, that:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><msubsup><mi>Φ</mi><mn>2</mn><mo>+</mo></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo>,</mo><msub><mi>K</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{E}[X_{i_1}X_{i_2}]=\Phi_2^+(\gamma,K_{V_{i_1}V_{i_2}})</annotation></semantics></math>
Hence, the best approach seems to be a middle path. A large time bin,
such as 20ms, results in robust autocorrelation estimates from the spike
data, but leads to unreliable dichotomized Gaussian simulations. A small
time bin, such as 1ms, leads to unreliable empirical autocorrelation
estimates, but reliable dichotomized Gaussian simulations. The default
(10ms) falls in the middle. Whatever the chosen bin size, a rolling mean
is taken to smooth the empirical autocorrelation value.</p>
<h3>
Example autocorrelation calculations
</h3>
<p>The function <a href="../reference/compute.autocorr.html">compute.autocorr</a> takes a
single neuron and computes its empirical autocorrelation using its spike
raster. Here, for example, is the function used to compute the raw
autocorrelation for the neuron with high autocorrelation shown
above:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/compute.autocorr.html">compute.autocorr</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span>, use_raw <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/plot-autocorrelation.html">plot.autocorrelation</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/compute_autocorrelation_high-1.png" width="700"></p>
<p>Which type of correlation, raw or Peason, is calculated is controlled
by the <strong>use_raw</strong> option. The Pearson autocorrelation for
the same data can be got by setting <strong>use_raw</strong> to
FALSE:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/compute.autocorr.html">compute.autocorr</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span>, use_raw <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/plot-autocorrelation.html">plot.autocorrelation</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/compute_autocorrelation_high_pearson-1.png" width="700"></p>
<p>As another example, here is the raw autocorrelation for the neuron
with low autocorrelation shown above:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/compute.autocorr.html">compute.autocorr</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_low</span><span class="op">]</span><span class="op">]</span>, use_raw <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/plot-autocorrelation.html">plot.autocorrelation</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_low</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/compute_autocorrelation_low-1.png" width="700"></p>
<p>And the Pearson autocorrelation for the same data:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/compute.autocorr.html">compute.autocorr</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_low</span><span class="op">]</span><span class="op">]</span>, use_raw <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/plot-autocorrelation.html">plot.autocorrelation</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_low</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/compute_autocorrelation_low_pearson-1.png" width="700"></p>
<h2>
Modeling autocorrelation decay
</h2>
<p>Theoretically, autocorrelation can be expected to exhibit exponential
decay over increasing lag, at least in cases with nonzero
autocorrelation. As noted above, this decay can be modeled with the
function:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>=</mo><mi>A</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mi>l</mi><mi>/</mi><mi>τ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">R = A\exp(-l/\tau) + b</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
is the <em>amplitude</em> (autocorrelation at the initial lag),
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
is lag,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>
is the <em>network time constant</em>, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>
is a constant (bias or baseline) term. The neurons package assumes that
the bias term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>
is a constant function of the firing rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
and bin size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Δ</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math>,
given as:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>λ</mi><mrow></mrow><mi>Δ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">b = (\lambda{}\Delta)^2</annotation></semantics></math>
In this formula both
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Δ</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math>
must be in the same unit of time, e.g., ms. The values for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>
are set by minimizing the mean squared error between the empirical
autocorrelation and the model function, using the <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#low-storage-bfgs" class="external-link">L-BFGS
algorithm as implemented in NLopt</a>.</p>
<p>Model fitting is accessed in the neurons package with the function <a href="../reference/fit.edf.autocorr.html">fit.edf.autocorr</a>. The
function takes a single neuron and fits the exponential decay function
to its empirical autocorrelation. Here, for example, is the function
used to fit the model to the raw autocorrelation for the neuron with
high autocorrelation:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/compute.autocorr.html">compute.autocorr</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span><span class="op">)</span> <span class="co"># Recompute with raw autocorrelation</span></span>
<span><span class="fu"><a href="../reference/fit.edf.autocorr.html">fit.edf.autocorr</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/plot-autocorrelation.html">plot.autocorrelation</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/fit_autocorrelation_high-1.png" width="700"></p>
<p>The fitted model is plotted as the red line. The parameters of the
exponential decay fit can be fetched directly with a neuron method and
provide succinct quantification of the empirical autocorrelation.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="fu">fetch_EDF_parameters</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##           A         tau   bias_term </span></span>
<span><span class="co">##  0.12535480 73.56534654  0.01105918</span></span></code></pre>
<p>In this case, the time constant tau is estimated to be 73.6ms and the
initial autocorrelation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
is estimated to be 0.125.</p>
<p>The above shows empirical autocorrelation and model fit being
performed in separate steps and for only one neuron at a time. The
function <a href="../reference/process.autocorr.html">process.autocorr</a> will
perform both steps at once for an entire list of neurons and return the
results in a data frame.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">autocor.results.batch</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/process.autocorr.html">process.autocorr</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">autocor.results.batch</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##       cell    lambda_ms  lambda_bin           A      tau    bias_term  autocorr1 max_autocorr mean_autocorr min_autocorr</span></span>
<span><span class="co">## 1 neuron_1 0.0021196442 0.021196442 0.012313504 38.18340 4.492891e-04 0.01276279  0.009925672  7.198735e-04 4.492891e-04</span></span>
<span><span class="co">## 2 neuron_2 0.0003823936 0.003823936 0.002640528 33.41566 1.462249e-05 0.00265515  0.001972220  6.441845e-05 1.462249e-05</span></span>
<span><span class="co">## 3 neuron_3 0.0036099494 0.036099494 0.020566831 71.32338 1.303173e-03 0.02187000  0.019179426  2.196287e-03 1.303173e-03</span></span>
<span><span class="co">## 4 neuron_4 0.0017867616 0.017867616 0.010491524 49.17751 3.192517e-04 0.01081078  0.008880300  6.233471e-04 3.192517e-04</span></span>
<span><span class="co">## 5 neuron_5 0.0096557914 0.096557914 0.075028474 31.83775 9.323431e-03 0.08435191  0.064128086  1.065232e-02 9.323431e-03</span></span>
<span><span class="co">## 6 neuron_6 0.0087146980 0.087146980 0.044857742 39.71559 7.594596e-03 0.05245234  0.042467352  8.618560e-03 7.594596e-03</span></span></code></pre>
<h2>
Estimating network time constants
</h2>
<p>The above discussion concerns computing and modeling
<em>empirical</em> autocorrelation, i.e., autocorrelation as computed
directly off a finite sample. However, what’s usually desired is an
estimate of the <em>population</em> value, i.e., the true
autocorrelation exhibited by a population of cells defined by some
shared covariate value. Estimating this true value is done by taking an
infinite sample, sampling not just all existing population members, but
also all possible members. This is, of course, impossible. However, it
can be approximated by taking larger and larger samples. The ideal, of
course, would be to take samples of the actual population, e.g.,
recording more cells, or recording more trials from the same cells.
However, in practice this is not possible. Instead, mathematical
techniques are used to simulate larger samples from existing data. The
most well-known technique is bootstrapping, i.e., “resampling” the
observed data with replacement. Bootstrapping does a good job perserving
the underlying statistical distribution of data when the signal-to-noise
ratio is high, but when the signal-to-noise ratio is low, bootstrapping
will simply amplify the noise. This is the case with autocorrelation
estimated from spike rasters, especially when the firing rate is low and
the recording time is short.</p>
<h3>
Dichotomized Gaussians
</h3>
<p>Instead of resampling with replacement, an alternative approach is to
simulate new samples through a random-process model that’s constrained
to be consistent with the observed data. In the case of exponentially
decaying autocorrelation, dichotomized Gaussians provide an ideal model.
The basic idea is to model the noisy processes underlying neuron spiking
across time as a latent multivariate Gaussian process, with one Gaussian
distribution per time bin. On this model, autocorrelation is modeled as
correlation between these Gaussians.</p>
<p>Consider the following example of a dichotomized Gaussian. First,
let’s draw a random sample of 300 points from a bivariate Gaussian
distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math>,
such that both component distributions
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mn>1</mn></msub><annotation encoding="application/x-tex">V_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mn>2</mn></msub><annotation encoding="application/x-tex">V_2</annotation></semantics></math>
are normal (i.e., have mean
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math>
of 0 and standard deviation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>
of 1) and such that a covariance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mi>V</mi></msub><annotation encoding="application/x-tex">K_V</annotation></semantics></math>
of 0.75 exists between these distributions.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">V_sample</span> <span class="op">&lt;-</span> <span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/mvrnorm.html" class="external-link">mvrnorm</a></span><span class="op">(</span></span>
<span>    n <span class="op">=</span> <span class="fl">300</span>,</span>
<span>    mu <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span><span class="op">)</span>,</span>
<span>    Sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span> </span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0.75</span>, </span>
<span>          <span class="fl">0.75</span>, <span class="fl">1</span><span class="op">)</span>, </span>
<span>        nrow <span class="op">=</span> <span class="fl">2</span>, </span>
<span>        ncol <span class="op">=</span> <span class="fl">2</span></span>
<span>      <span class="op">)</span> </span>
<span>  <span class="op">)</span></span></code></pre></div>
<p>Next, let’s plot these points and superimpose on top of them
thresholds
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma = 1</annotation></semantics></math>
for both dimensions, shading the area of points below the threshold.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Convert to data frame for plotting</span></span>
<span><span class="va">V_sample</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">V_sample</span><span class="op">)</span> </span>
<span><span class="va">threshold</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="co"># Make and print plot</span></span>
<span><span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">V_sample</span>, <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">V_sample</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, y <span class="op">=</span> <span class="va">V_sample</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">labs</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="st">"V1"</span>, </span>
<span>    y <span class="op">=</span> <span class="st">"V2"</span>, </span>
<span>    title <span class="op">=</span> <span class="st">"Example bivariate data"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html" class="external-link">ylim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">4.5</span>,<span class="fl">4.5</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html" class="external-link">xlim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">4.5</span>,<span class="fl">4.5</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html" class="external-link">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html" class="external-link">theme</a></span><span class="op">(</span></span>
<span>    panel.background <span class="op">=</span> <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html" class="external-link">element_rect</a></span><span class="op">(</span>fill <span class="op">=</span> <span class="st">"white"</span>, colour <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span>,</span>
<span>    plot.background  <span class="op">=</span> <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html" class="external-link">element_rect</a></span><span class="op">(</span>fill <span class="op">=</span> <span class="st">"white"</span>, colour <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html" class="external-link">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">0</span>, color <span class="op">=</span> <span class="st">"darkgray"</span>, linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html" class="external-link">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span>, color <span class="op">=</span> <span class="st">"darkgray"</span>, linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html" class="external-link">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="va">threshold</span>, color <span class="op">=</span> <span class="st">"darkblue"</span>, linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html" class="external-link">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="va">threshold</span>, color <span class="op">=</span> <span class="st">"darkblue"</span>, linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html" class="external-link">annotate</a></span><span class="op">(</span></span>
<span>    <span class="st">"rect"</span>, </span>
<span>    xmin <span class="op">=</span> <span class="op">-</span><span class="cn">Inf</span>, xmax <span class="op">=</span> <span class="va">threshold</span>, </span>
<span>    ymin <span class="op">=</span> <span class="op">-</span><span class="cn">Inf</span>, ymax <span class="op">=</span> <span class="va">threshold</span>, </span>
<span>    fill <span class="op">=</span> <span class="st">"lightblue"</span>, alpha <span class="op">=</span> <span class="fl">0.3</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html" class="external-link">annotate</a></span><span class="op">(</span></span>
<span>    <span class="st">"text"</span>, x <span class="op">=</span> <span class="va">threshold</span> <span class="op">+</span> <span class="fl">0.35</span>, y <span class="op">=</span> <span class="fl">4.5</span>, </span>
<span>    label <span class="op">=</span> <span class="st">"gamma"</span>, parse <span class="op">=</span> <span class="cn">TRUE</span>, color <span class="op">=</span> <span class="st">"darkblue"</span>, </span>
<span>    size <span class="op">=</span> <span class="fl">7</span>, hjust <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html" class="external-link">annotate</a></span><span class="op">(</span></span>
<span>    <span class="st">"text"</span>, x <span class="op">=</span> <span class="fl">4.5</span>, y <span class="op">=</span> <span class="va">threshold</span> <span class="op">+</span> <span class="fl">0.35</span>, </span>
<span>    label <span class="op">=</span> <span class="st">"gamma"</span>, parse <span class="op">=</span> <span class="cn">TRUE</span>, color <span class="op">=</span> <span class="st">"darkblue"</span>, </span>
<span>    size <span class="op">=</span> <span class="fl">7</span>, vjust <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/DG_plot-1.png" width="700"></p>
<p>Notice how a threshold value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>
defines, for any
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mi>i</mi></msub><annotation encoding="application/x-tex">V_i</annotation></semantics></math>,
a new binary random variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
such that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">X=1</annotation></semantics></math>
if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>i</mi></msub><mo>&gt;</mo><mi>γ</mi></mrow><annotation encoding="application/x-tex">V_i&gt;\gamma</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">X=0</annotation></semantics></math>
otherwise. The variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
is the “dichotomized Gaussian”.</p>
<h3>
Simulating spike rate
</h3>
<p>If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
is a dichotomized Gaussian, the probability
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>=</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(X=1)</annotation></semantics></math>
that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
is 1 is given by the cumulative distribution function of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mi>i</mi></msub><annotation encoding="application/x-tex">V_i</annotation></semantics></math>
evaluated at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>.
As each
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mi>i</mi></msub><annotation encoding="application/x-tex">V_i</annotation></semantics></math>
is stipulated to be a standard normal
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu=0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sigma=1</annotation></semantics></math>),
this cumulative distribution function is the standard normal cumulative
distribution function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Φ</mi><annotation encoding="application/x-tex">\Phi</annotation></semantics></math>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><msqrt><mrow><mn>2</mn><mi>π</mi></mrow></msqrt></mfrac><msubsup><mo>∫</mo><mrow><mi>−</mi><mi>∞</mi></mrow><mi>x</mi></msubsup><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><msup><mi>t</mi><mn>2</mn></msup><mi>/</mi><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">\Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x \exp(-t^2/2) dt</annotation></semantics></math>
Thus:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>=</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>V</mi><mi>i</mi></msub><mo>&gt;</mo><mi>γ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>−</mo><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(X=1) = P(V_i&gt;\gamma) = 1 - \Phi(\gamma)</annotation></semantics></math>
In the above plot, the function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Phi(\gamma)</annotation></semantics></math>
corresponds to the shaded area of each axis, while
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>=</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">1 - \Phi(\gamma)=P(X=1)</annotation></semantics></math>
corresponds to the non-shaded area.</p>
<p>It follows that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mi>i</mi></msub><annotation encoding="application/x-tex">V_i</annotation></semantics></math>
can be used to simulate a neuron with mean spike rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
by setting the threshold
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>
such that:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>1</mn><mo>−</mo><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\lambda = 1 - \Phi(\gamma)</annotation></semantics></math>
which means that:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>=</mo><msup><mi>Φ</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\gamma = \Phi^{-1}(1-\lambda)</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Φ</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\Phi^{-1}</annotation></semantics></math>
is the inverse of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Φ</mi><annotation encoding="application/x-tex">\Phi</annotation></semantics></math>,
i.e., is the quantile function. This quantile function can be computed
via well-known numerical approximations, meaning that a dichotomized
Gaussian can easily be used to simulate a neuron with a desired mean
spike rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>.</p>
<h3>
Simulating autocorrelation
</h3>
<p>While simulating a given spike rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
is straightforward, simulating autocorrelation is not. How is
autocorrelation to be represented in the model? If the correlation
between two component dimensions
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><annotation encoding="application/x-tex">V_{i_1}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub><annotation encoding="application/x-tex">V_{i_2}</annotation></semantics></math>
of a multivariate Gaussian is to represent the autocorrelation between
two time bins
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><annotation encoding="application/x-tex">X_{i_1}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub><annotation encoding="application/x-tex">X_{i_2}</annotation></semantics></math>
of a spike raster separated by lag
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>,
then each
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mi>i</mi></msub><annotation encoding="application/x-tex">V_i</annotation></semantics></math>
should not be thought of as a separate neuron, but rather as the same
neuron at different time points.</p>
<p>While this is an insightful idea, the operation of thresholding,
needed to convert a Gaussian variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mi>i</mi></msub><annotation encoding="application/x-tex">V_i</annotation></semantics></math>
into a simulated binary spike variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding="application/x-tex">X_i</annotation></semantics></math>,
will change the correlation:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo>≠</mo><msub><mi>R</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub></mrow><annotation encoding="application/x-tex">R_{V_{i_1}V_{i_2}} \neq R_{X_{i_1}X_{i_2}}</annotation></semantics></math>
However, all is not lost. Given some autocorrelation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">R_{X_{i_1}X_{i_2}}</annotation></semantics></math>
between two time bins
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><annotation encoding="application/x-tex">X_{i_1}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub><annotation encoding="application/x-tex">X_{i_2}</annotation></semantics></math>
separated by lag
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
for a neuron
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
with spike rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>,
a correlation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">R_{V_{i_1}V_{i_2}}</annotation></semantics></math>
can often be found which, when these variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mi>i</mi></msub><annotation encoding="application/x-tex">V_i</annotation></semantics></math>
are thresholded by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>=</mo><msup><mi>Φ</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>λ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\gamma = \Phi^{-1}(1-\lambda)</annotation></semantics></math>,
gives back the original correlation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">R_{X_{i_1}X_{i_2}}</annotation></semantics></math>.
Actually, in this case, it’s the covariance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">K_{V_{i_1}V_{i_2}}</annotation></semantics></math>
that is sought. Knowing the correlation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">R_{V_{i_1}V_{i_2}}</annotation></semantics></math>
is not critical, although because
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math>
is normal,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo>=</mo><msub><mi>K</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub></mrow><annotation encoding="application/x-tex">R_{V_{i_1}V_{i_2}} = K_{V_{i_1}V_{i_2}}</annotation></semantics></math>.</p>
<p>By definition, the covariance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">K_{X_{i_1}X_{i_2}}</annotation></semantics></math>
is given by:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo>=</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub><mo stretchy="true" form="postfix">]</mo></mrow><mo>−</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><mo stretchy="true" form="postfix">]</mo></mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">K_{X_{i_1}X_{i_2}}= \text{E}[X_{i_1}X_{i_2}] - \text{E}[X_{i_1}]\text{E}[X_{i_2}]</annotation></semantics></math>
For all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>V</mi><mi>i</mi></msub><mo>&gt;</mo><mi>γ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>−</mo><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{E}[X_{i}] = P(V_{i}&gt;\gamma)=1-\Phi(\gamma)</annotation></semantics></math>
Further, notice that the expected value of the product
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow><annotation encoding="application/x-tex">X_{i_1}X_{i_2}</annotation></semantics></math>
is given by the upper-tail cumulative distribution function of a
bivariate normal distribution with covariance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">K_{V_{i_1}V_{i_2}}</annotation></semantics></math>,
evaluated at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>
for both components:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><msubsup><mi>Φ</mi><mn>2</mn><mo>+</mo></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo>,</mo><msub><mi>K</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{E}[X_{i_1}X_{i_2}] = \Phi_2^+(\gamma,K_{V_{i_1}V_{i_2}})</annotation></semantics></math>
For a threshold
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Φ</mi><mn>2</mn><mo>+</mo></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>K</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Phi_2^+(x,K_{V_{i_1}V_{i_2}})</annotation></semantics></math>
is the probability that both components of a bivariate normal
distribution with covariance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">K_{V_{i_1}V_{i_2}}</annotation></semantics></math>
are greater than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msubsup><mi>Φ</mi><mn>2</mn><mo>+</mo></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>K</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>V</mi><mn>1</mn></msub><mo>&gt;</mo><mi>x</mi><mo>,</mo><msub><mi>V</mi><mn>2</mn></msub><mo>&gt;</mo><mi>x</mi><mspace width="0.278em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.278em"></mspace><msub><mi>V</mi><mn>1</mn></msub><mo>,</mo><msub><mi>V</mi><mn>2</mn></msub><mo>∼</mo><mtext mathvariant="normal">MVN</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>=</mo><mn>0</mn><mo>,</mo><mi>σ</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>K</mi><mo>=</mo><msub><mi>K</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mn>1</mn><msqrt><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>π</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><mi>𝐝</mi><mi>𝐞</mi><mi>𝐭</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>K</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msqrt></mfrac><msubsup><mo>∫</mo><mi>x</mi><mi>∞</mi></msubsup><msubsup><mo>∫</mo><mi>x</mi><mi>∞</mi></msubsup><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false" form="prefix">⟨</mo><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub><msup><mo stretchy="false" form="postfix">⟩</mo><mtext mathvariant="normal">T</mtext></msup><msubsup><mi>K</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msubsup><mo stretchy="false" form="prefix">⟨</mo><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub><mo stretchy="false" form="postfix">⟩</mo><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mo stretchy="false" form="prefix">⟨</mo><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub><mo stretchy="false" form="postfix">⟩</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
  \Phi_2^+(x,K_{V_{i_1}V_{i_2}}) 
    &amp;= P(V_1&gt;x, V_2&gt;x \;|\; V_1, V_2\sim \text{MVN}(\mu=0,\sigma=1,K=K_{V_{i_1}V_{i_2}})) \\
    &amp;= \frac{1}{\sqrt{(2\pi)^2\mathbf{det}(K_{V_{i_1}V_{i_2}})}} 
      \int_{x}^\infty \int_{x}^\infty \exp(-\frac{1}{2}\langle V_{i_1}V_{i_2}\rangle^\text{T}K_{V_{i_1}V_{i_2}}^{-1}\langle V_{i_1}V_{i_2}\rangle) d\langle V_{i_1}V_{i_2}\rangle
\end{align}</annotation></semantics></math> Putting it all together:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo>=</mo><msubsup><mi>Φ</mi><mn>2</mn><mo>+</mo></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo>,</mo><msub><mi>K</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">K_{X_{i_1}X_{i_2}} = \Phi_2^+(\gamma,K_{V_{i_1}V_{i_2}}) - (1-\Phi(\gamma))^2</annotation></semantics></math>
Thus, the covariance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">K_{V_{i_1}V_{i_2}}</annotation></semantics></math>
between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><annotation encoding="application/x-tex">V_{i_1}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub><annotation encoding="application/x-tex">V_{i_2}</annotation></semantics></math>
which, after dichotomization, yields covariance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">K_{X_{i_1}X_{i_2}}</annotation></semantics></math>,
can be found by solving for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">K_{V_{i_1}V_{i_2}}</annotation></semantics></math>
in the equation:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>=</mo><msub><mi>K</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo>−</mo><msubsup><mi>Φ</mi><mn>2</mn><mo>+</mo></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo>,</mo><msub><mi>K</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">0=K_{X_{i_1}X_{i_2}} - \Phi_2^+(\gamma,K_{V_{i_1}V_{i_2}}) + (1-\Phi(\gamma))^2</annotation></semantics></math>
The neurons package solves this equation for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">K_{V_{i_1}V_{i_2}}</annotation></semantics></math>
using a root bisection algorithm.</p>
<p>The only remaining task is to determine the covariance needed for a
given desired autocorrelation. This is straightforward from the
definitions of each type of correlation. For raw autocorrelation:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo>=</mo><msub><mi>R</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo>−</mo><msup><mi>λ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">K_{X_{i_1}X_{i_2}} = R_{X_{i_1}X_{i_2}} - \lambda^2</annotation></semantics></math>for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
in time units of bin. For Pearson autocorrelation:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo>=</mo><msub><mi>R</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><msubsup><mi>σ</mi><mi>X</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">K_{X_{i_1}X_{i_2}} = R_{X_{i_1}X_{i_2}}\sigma_X^2</annotation></semantics></math>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mi>X</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma_X^2</annotation></semantics></math>
is the variance of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>,
again in bin units.</p>
<h3>
Running and bootstrapping simulations
</h3>
<p>The function <a href="../reference/estimate.autocorr.params.html">estimate.autocorr.params</a>
takes a list of neurons and:</p>
<ol style="list-style-type: decimal">
<li>Computes the empirical autocorrelation of each neuron.</li>
<li>Fits an exponential decay model to that empirical
autocorrelation.</li>
<li>Generates many simulated spike trains (dichotomized Gaussians) based
on the values
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><mi>X</mi><mi>X</mi></mrow></msub><annotation encoding="application/x-tex">R_{XX}</annotation></semantics></math>
predicted by the model of the empirical autocorrelation and the observed
firing rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
of the neuron.</li>
<li>Computes the empirical autocorrelation of each simulated spike
train.</li>
<li>Fits an exponential decay model to the empirical autocorrelation of
each simulated spike train.</li>
</ol>
<p>This procedure yields a distribution of possible
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>
values for each neuron. For the purpose of speed, this tutorial runs
only 100 simulations per neuron, but in practice, 1000 or more
simulations should be run.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">autocor.ests</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/estimate.autocorr.params.html">estimate.autocorr.params</a></span><span class="op">(</span></span>
<span>    neuron_list <span class="op">=</span> <span class="va">neurons</span>,</span>
<span>    n_trials_per_sim <span class="op">=</span> <span class="fl">500</span>, </span>
<span>    n_sims_per_neurons <span class="op">=</span> <span class="fl">100</span>,</span>
<span>    use_raw <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>  <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">autocor.ests</span><span class="op">$</span><span class="va">estimates</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##    lambda_ms lambda_bin          A      tau    bias_term  autocorr1 max_autocorr mean_autocorr min_autocorr</span></span>
<span><span class="co">## 1 0.01688889  0.1688889 0.01190413 38.56055 0.0002852346 0.01218937  0.009470064  0.0005497602 0.0002852346</span></span>
<span><span class="co">## 2 0.01590850  0.1590850 0.01092675 40.61636 0.0002530803 0.01117983  0.008795191  0.0005105874 0.0002530803</span></span>
<span><span class="co">## 3 0.01878431  0.1878431 0.01418505 36.24284 0.0003528504 0.01453790  0.011117548  0.0006465601 0.0003528504</span></span>
<span><span class="co">## 4 0.01671895  0.1671895 0.01203930 36.50089 0.0002795234 0.01231883  0.009433704  0.0005308353 0.0002795234</span></span>
<span><span class="co">## 5 0.01854902  0.1854902 0.01405934 37.42106 0.0003440661 0.01440341  0.011106459  0.0006460043 0.0003440661</span></span>
<span><span class="co">## 6 0.01822222  0.1822222 0.01333629 37.17653 0.0003320494 0.01366834  0.010523018  0.0006163265 0.0003320494</span></span></code></pre>
<p>With the simulations run, the final step is to estimate the network
time constant for covariates of interest. The function <a href="../reference/analyze.autocorr.html">analyze.autocorr</a> does this
by bootstrapping over the tau values obtained from the simulations. If
there are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
neurons in a covariate level,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
simulations have been run per neuron, then each bootstrap resample
consists of the mean of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
draws with replacement from the pool of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">nm</annotation></semantics></math>
values for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>.
For this tutorial, 10k bootstrap resamples are used.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">autocor.results.bootstraps</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/analyze.autocorr.html">analyze.autocorr</a></span><span class="op">(</span></span>
<span>    <span class="va">autocor.ests</span>,</span>
<span>    covariate <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"hemi"</span>,<span class="st">"genotype"</span><span class="op">)</span>,</span>
<span>    n_bs <span class="op">=</span> <span class="fl">1e4</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
<p>The function <a href="../reference/analyze.autocorr.html">analyze.autocorr</a> returns a
list with two objects. The first is <strong>resamples</strong>, a
dataframe holding the tau values for each covariate from each
simulation.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">autocor.results.bootstraps</span><span class="op">$</span><span class="va">resamples</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##   LH_CBA/J  RH_CBA/J</span></span>
<span><span class="co">## 1 57.97739  80.89738</span></span>
<span><span class="co">## 2 57.74824 108.46566</span></span>
<span><span class="co">## 3 57.34851  76.86553</span></span>
<span><span class="co">## 4 56.12538  89.17201</span></span>
<span><span class="co">## 5 56.71212  95.08618</span></span>
<span><span class="co">## 6 57.44959  81.51360</span></span></code></pre>
<p>The second is <strong>distribution_plot</strong>, a ggplot2 object
visualizing the bootstrap distributions of tau for each covariate.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">autocor.results.bootstraps</span><span class="op">$</span><span class="va">distribution_plot</span><span class="op">)</span></span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/plot_distribution-1.png" width="700"></p>
<h2>
Code summary
</h2>
<p>The essential steps to run this analysis are as follows:</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Setup</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/rm.html" class="external-link">rm</a></span><span class="op">(</span>list <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ls.html" class="external-link">ls</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">12345</span><span class="op">)</span> </span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Load demo data</span></span>
<span><span class="va">spike.rasters</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html" class="external-link">read.csv</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span></span>
<span>      <span class="st">"extdata"</span>, </span>
<span>      <span class="st">"spike_rasters_2022data.csv"</span>, </span>
<span>      package <span class="op">=</span> <span class="st">"neurons"</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Create neuron objects</span></span>
<span><span class="va">neurons</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load.rasters.as.neurons.html">load.rasters.as.neurons</a></span><span class="op">(</span><span class="va">spike.rasters</span>, bin_size <span class="op">=</span> <span class="fl">10.0</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Run dichotomized Gaussian simulations </span></span>
<span><span class="va">autocor.ests</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/estimate.autocorr.params.html">estimate.autocorr.params</a></span><span class="op">(</span></span>
<span>    neuron_list <span class="op">=</span> <span class="va">neurons</span>,</span>
<span>    n_trials_per_sim <span class="op">=</span> <span class="fl">500</span>, </span>
<span>    n_sims_per_neurons <span class="op">=</span> <span class="fl">100</span>,</span>
<span>    use_raw <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Analyze results by covariate</span></span>
<span><span class="va">autocor.results.bootstraps</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/analyze.autocorr.html">analyze.autocorr</a></span><span class="op">(</span></span>
<span>    <span class="va">autocor.ests</span>,</span>
<span>    covariate <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"hemi"</span>,<span class="st">"genotype"</span><span class="op">)</span>,</span>
<span>    n_bs <span class="op">=</span> <span class="fl">1e4</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Michael Barkasi.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
