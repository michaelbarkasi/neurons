<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Network time constants via dichotomized Gaussians ‚Ä¢ neurons</title>
<script src="../lightswitch.js"></script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="Network time constants via dichotomized Gaussians">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top " aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">neurons</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../index.html">Introduction</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Package Functions</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Installation</h6></li>
    <li><a class="dropdown-item" href="../articles/install.html">Installation instructions</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Tutorials</h6></li>
    <li><a class="dropdown-item" href="../articles/tutorial_synchrony_crosscorr.html">Spike synchrony via cross-correlation</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial_tau_est_DG.html">Network time constants via dichotomized Gaussians</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial_tau_est_kilosort.html">Network time constants from KiloSort4 data</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Legal</h6></li>
    <li><a class="dropdown-item" href="../articles/license.html">Dependency licenses</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/michaelbarkasi/neurons"><span class="fa fab fa-github"></span></a></li>
<li class="nav-item"><a class="nav-link" href="../#">üåô</a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Network time constants via dichotomized Gaussians</h1>
            
      

      <div class="d-none name"><code>tutorial_tau_est_DG.Rmd</code></div>
    </div>

    
    
<p>The extent to which an individual neuron feeds back on itself in a
recurrent loop can be estimated by its autocorrelation, i.e., the
correlation between the neuron‚Äôs membrane potential
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>
at time
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding="application/x-tex">t_1</annotation></semantics></math>
and at a later time
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mn>2</mn></msub><mo>&gt;</mo><msub><mi>t</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">t_2&gt;t_1</annotation></semantics></math>.
The more a spike <em>now</em> increases the probability of a spike
<em>later</em>, the stronger the neuron‚Äôs connection back onto itself. A
neuron‚Äôs autocorrelation, represented by the variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>,
can be modeled with an exponential decay function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>=</mo><mi>A</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>‚àí</mi><mi>l</mi><mi>/</mi><mi>œÑ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">R = A\exp(-l/\tau) + b</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
is the <em>amplitude</em> (autocorrelation at the initial lag),
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
is lag,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÑ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>
is the <em>network time constant</em>, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>
is a constant (bias or baseline) term. The time constant
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÑ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>
is a measure of how quickly the neuron‚Äôs autocorrelation decays back to
baseline after a spike.</p>
<p>Network time constants are difficult to estimate from experimental
data, as demonstrated in <a href="tutorial_tau_est_kilosort.html">the
tutorial on estimating them from Kilosort4 data</a>. The spiking
activity of a neuron is indicative of its recurrence only if that neuron
is receiving no other input. Thus, time constants must be estimated from
periods of spontaneous activity. These periods are short and noisy,
making time constant estimates from empirical calculations of a neuron‚Äôs
autocorrelation unreliable. A way to improve the signal-to-noise ratio
is needed, such as simulating many recordings. However, typical
approaches to such simulations, such as bootstrapping, will only amplify
the noise. A better approach is to use dichotomized Gaussians.</p>
<p>This tutorial shows how to use the neurons package to estimate
network time constants using dichotomized Gaussians. Patch-clamp
recordings will be used as an example dataset. The recordings are from
layer 2/3 of the auditory cortex of mature wildtype mice, in both the
left and right hemisphere. These recordings are used by <a href="https://doi.org/10.1371/journal.pbio.3001803" class="external-link">Neophytou et
al.¬†2022</a>, who adapt and apply the dichotomized Gaussian approach of
<a href="https://doi.org/10.1162/neco.2008.02-08-713" class="external-link">Macke et
al.¬†2009</a> to show that the right auditory cortex of mice has more
recurrence than the left. This tutorial reproduces that analysis, with a
few minor tweaks and improvements.</p>
<h2>
Load spike rasters
</h2>
<p>Set up the R environment by clearing the workspace, setting a
random-number generator seed, and loading the neurons package.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Clear the R workspace to start fresh</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/rm.html" class="external-link">rm</a></span><span class="op">(</span>list <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ls.html" class="external-link">ls</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Set seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">12345</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Load neurons package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">)</span> </span></code></pre></div>
<p>All of the data is contained in a single csv file, provided with the
neurons package, as a compact spike raster.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">spike.rasters</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html" class="external-link">read.csv</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span></span>
<span>      <span class="st">"extdata"</span>, </span>
<span>      <span class="st">"spike_rasters_2022data.csv"</span>, </span>
<span>      package <span class="op">=</span> <span class="st">"neurons"</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">spike.rasters</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##   trial sample cell time_in_ms recording_name hemi genotype sex    age region</span></span>
<span><span class="co">## 1     2   1181    1      118.1    20080930002   LH    CBA/J   M P30:60    ACx</span></span>
<span><span class="co">## 2     3   1286    1      128.6    20080930002   LH    CBA/J   M P30:60    ACx</span></span>
<span><span class="co">## 3     3  13537    1     1353.7    20080930002   LH    CBA/J   M P30:60    ACx</span></span>
<span><span class="co">## 4     4    691    1       69.1    20080930002   LH    CBA/J   M P30:60    ACx</span></span>
<span><span class="co">## 5     4   2404    1      240.4    20080930002   LH    CBA/J   M P30:60    ACx</span></span>
<span><span class="co">## 6     4   3746    1      374.6    20080930002   LH    CBA/J   M P30:60    ACx</span></span></code></pre>
<p>The data takes the form of a dataframe the rows of which each
represent a single recorded spike. Each column gives relevant metadata,
such as the time in the recording of the spike, the identity of the
neuron that fired the spike, and the hemisphere in which that neuron was
recorded. The function <a href="../reference/load.rasters.as.neurons.html">load.rasters.as.neurons</a>
will convert a compact raster of spikes like this one (a dataframe or
file name to a csv importable as such) into <strong>neuron</strong>
objects (one per cell), so long as it has the recognized columns:
<strong>cell</strong>, <strong>time_in_ms</strong>, and
<strong>trial</strong>. If the optional columns
<strong>recording_name</strong>, <strong>hemisphere</strong>,
<strong>genotype</strong>, <strong>sex</strong>,
<strong>region</strong>, or <strong>age</strong> are included, they will
be recognized and added as metadata to the neuron objects.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">neurons</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load.rasters.as.neurons.html">load.rasters.as.neurons</a></span><span class="op">(</span><span class="va">spike.rasters</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Number of cells discovered:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Number of cells discovered: 41</span></span></code></pre>
<p>The <strong>neuron</strong> object class is native to C++ and
integrated into neurons (an R package) via Rcpp. It comes with built-in
methods for plotting rasters, plotting autocorrelation, and estimating
autocorrelation parameters with dichotomized Gaussian simulations. Some
of these methods can be accessed through R, but neurons provides
R-native wrappers for the most useful ones. The neurons package also
provides native R functions for plotting. Let‚Äôs plot the rasters for two
cells. The first has high autocorrelation, as can be seen from the long
horizontal streaks of spikes:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cell_high</span> <span class="op">&lt;-</span> <span class="fl">16</span></span>
<span><span class="fu"><a href="../reference/plot-raster.html">plot.raster</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span><span class="op">)</span> </span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/plot_raster_high_autocor-1.png" width="700"></p>
<p>The second has low autocorrelation, as can be seen from the more
random distribution of spikes without long streaks:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cell_low</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="fu"><a href="../reference/plot-raster.html">plot.raster</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_low</span><span class="op">]</span><span class="op">]</span><span class="op">)</span> </span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/plot_raster_low_autocor-1.png" width="700"></p>
<h2>
Computing empirical autocorrelation from data
</h2>
<p>There are two common definitions for the correlation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><annotation encoding="application/x-tex">R_{XY}</annotation></semantics></math>
between two random variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>.
Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\text{E}[X]</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\text{E}[Y]</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mi>Y</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\text{E}[XY]</annotation></semantics></math>
be the expected values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>,
and their product
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>Y</mi></mrow><annotation encoding="application/x-tex">XY</annotation></semantics></math>.
The first definition, sometimes called the <em>raw correlation</em>, is:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo>=</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mi>Y</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">R_{XY} = \text{E}[XY]</annotation></semantics></math>
The second, sometimes called the <em>Pearson correlation</em>, centers
and normalizes the raw correlation:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo>=</mo><mfrac><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mi>Y</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>‚àí</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mo stretchy="true" form="postfix">]</mo></mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><mrow><msqrt><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>X</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">]</mo></mrow><mo>‚àí</mo><mtext mathvariant="normal">E</mtext><msup><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mo stretchy="true" form="postfix">]</mo></mrow><mn>2</mn></msup></mrow></msqrt><msqrt><mrow><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>Y</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">]</mo></mrow><mo>‚àí</mo><mtext mathvariant="normal">E</mtext><msup><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mo stretchy="true" form="postfix">]</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mfrac></mrow><annotation encoding="application/x-tex">R_{XY} = \frac{\text{E}[XY] - \text{E}[X]\text{E}[Y]}{\sqrt{\text{E}[X^2] - \text{E}[X]^2}\sqrt{\text{E}[Y^2] - \text{E}[Y]^2}}</annotation></semantics></math>
While only the Pearson correlation will return values constrained to the
range
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>‚àí</mi><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[-1,1]</annotation></semantics></math>,
the raw correlation is well-defined for a broader range of cases,
including cases where empirical estimates need to be made from
observations with little to no variance, such as spike rasters with low
firing rates.</p>
<p>The above equations are for theoretical <em>population</em> values,
given in terms of the expected value operator. Both definitions have
empirical analogues which can be used to calculate a value directly from
a finite sample. If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>X</mi><mo accent="true">‚Üí</mo></mover><mo>=</mo><mo stretchy="false" form="prefix">‚ü®</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>X</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">‚ü©</mo></mrow><annotation encoding="application/x-tex">\vec{X}=\langle X_1,\ldots,X_T\rangle</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Y</mi><mo accent="true">‚Üí</mo></mover><mo>=</mo><mo stretchy="false" form="prefix">‚ü®</mo><msub><mi>Y</mi><mn>1</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>Y</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">‚ü©</mo></mrow><annotation encoding="application/x-tex">\vec{Y}=\langle Y_1,\ldots,Y_T\rangle</annotation></semantics></math>
are each a series of samples collected from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>,
then the empirical raw correlation is given in terms of the dot product:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo>=</mo><mfrac><mrow><mover><mi>X</mi><mo accent="true">‚Üí</mo></mover><mo>‚ãÖ</mo><mover><mi>Y</mi><mo accent="true">‚Üí</mo></mover></mrow><mi>T</mi></mfrac><mo>=</mo><mfrac><mn>1</mn><mi>T</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><msub><mi>X</mi><mi>i</mi></msub><msub><mi>Y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">R_{XY} = \frac{\vec{X}\cdot\vec{Y}}{T} = \frac{1}{T}\sum_{i=1}^T X_i Y_i</annotation></semantics></math>
If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œº</mi><mover><mrow><mi>X</mi><mi>Y</mi></mrow><mo accent="true">‚Üí</mo></mover></msub><annotation encoding="application/x-tex">\mu_{\vec{XY}}</annotation></semantics></math>
is the empirical mean of the vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mrow><mi>X</mi><mi>Y</mi></mrow><mo accent="true">‚Üí</mo></mover><annotation encoding="application/x-tex">\vec{XY}</annotation></semantics></math>
of products
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><msub><mi>Y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">X_iY_i</annotation></semantics></math>,
the raw correlation can also be computed as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo>=</mo><msub><mi>Œº</mi><mover><mrow><mi>X</mi><mi>Y</mi></mrow><mo accent="true">‚Üí</mo></mover></msub></mrow><annotation encoding="application/x-tex">R_{XY} = \mu_{\vec{XY}}</annotation></semantics></math>.
The empirical Pearson correlation between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>X</mi><mo accent="true">‚Üí</mo></mover><annotation encoding="application/x-tex">\vec{X}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>Y</mi><mo accent="true">‚Üí</mo></mover><annotation encoding="application/x-tex">\vec{Y}</annotation></semantics></math>
with means
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œº</mi><mover><mi>X</mi><mo accent="true">‚Üí</mo></mover></msub><annotation encoding="application/x-tex">\mu_\vec{X}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œº</mi><mover><mi>Y</mi><mo accent="true">‚Üí</mo></mover></msub><annotation encoding="application/x-tex">\mu_\vec{Y}</annotation></semantics></math>
and standard deviations
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œÉ</mi><mover><mi>X</mi><mo accent="true">‚Üí</mo></mover></msub><annotation encoding="application/x-tex">\sigma_\vec{X}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œÉ</mi><mover><mi>Y</mi><mo accent="true">‚Üí</mo></mover></msub><annotation encoding="application/x-tex">\sigma_\vec{Y}</annotation></semantics></math>
is given by:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo>=</mo><mfrac><mrow><msub><mi>Œº</mi><mover><mrow><mi>X</mi><mi>Y</mi></mrow><mo accent="true">‚Üí</mo></mover></msub><mo>‚àí</mo><msub><mi>Œº</mi><mover><mi>X</mi><mo accent="true">‚Üí</mo></mover></msub><msub><mi>Œº</mi><mover><mi>Y</mi><mo accent="true">‚Üí</mo></mover></msub></mrow><mrow><msub><mi>œÉ</mi><mover><mi>X</mi><mo accent="true">‚Üí</mo></mover></msub><msub><mi>œÉ</mi><mover><mi>Y</mi><mo accent="true">‚Üí</mo></mover></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">R_{XY} = \frac{\mu_{\vec{XY}} - \mu_\vec{X}\mu_\vec{Y}}{\sigma_\vec{X} \sigma_\vec{Y}}</annotation></semantics></math>
For ease of reading, the vector notation will hereafter be dropped and
capital letters will be used to refer to both the random variables and
their empirical samples, with the understanding that the context will
make clear which is meant.</p>
<p>If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
is a time series
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mo stretchy="false" form="prefix">‚ü®</mo><mrow></mrow><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>X</mi><mi>t</mi></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>X</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">‚ü©</mo><mrow></mrow></mrow><annotation encoding="application/x-tex">X=\langle{}X_1, \ldots, X_t, \ldots, X_T\rangle{}</annotation></semantics></math>,
then its autocorrelation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>X</mi><mi>X</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">R_{XX}(l)</annotation></semantics></math>
is the correlation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><annotation encoding="application/x-tex">R_{XY}</annotation></semantics></math>
between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and a copy
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy="false" form="prefix">‚ü®</mo><mrow></mrow><msub><mi>X</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>X</mi><mrow><mi>l</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>,</mo><msub><mi>X</mi><mrow><mi>l</mi><mo>+</mo><mn>3</mn></mrow></msub><mo>,</mo><mi>‚Ä¶</mi><mo stretchy="false" form="postfix">‚ü©</mo><mrow></mrow></mrow><annotation encoding="application/x-tex">Y=\langle{}X_{l+1}, X_{l+2}, X_{l+3}, \ldots \rangle{}</annotation></semantics></math>
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
time shifted by some lag
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>.
This lagged copy will be shorter than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
samples, so computing the empirical autocorrelation requires adjusting
the summation index and the normalization term accordingly. For example,
the empirical raw autocorrelation at lag
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
is given by:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>X</mi><mi>X</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mi>T</mi><mo>‚àí</mo><mi>l</mi></mrow></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>‚àí</mo><mi>l</mi></mrow></munderover><msub><mi>X</mi><mi>i</mi></msub><msub><mi>Y</mi><mrow><mi>i</mi><mo>+</mo><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">R_{XX}(l) = \frac{1}{T-l}\sum_{i=1}^{T-l} X_i Y_{i+l}</annotation></semantics></math>
If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
are matrices containing many trials (columns) of sample series (rows) of
data collected from the random variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>,
then the empirical correlation between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
can be refined by averaging together the correlations computed for each
trial. For example, the empirical raw autocorrelation at lag
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
is given by:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>X</mi><mi>X</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mfrac><mn>1</mn><mrow><mi>T</mi><mo>‚àí</mo><mi>l</mi></mrow></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>‚àí</mo><mi>l</mi></mrow></munderover><msub><mi>X</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>Y</mi><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo>+</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">R_{XX}(l) = \frac{1}{N}\sum_{j=1}^N \frac{1}{T-l}\sum_{i=1}^{T-l} X_{ij} Y_{(i+l)j}</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
is the number of trials (columns) in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>.</p>
<p>The <strong>neuron</strong> object class has built-in methods for
computing both the raw and Pearson empirical autocorrelation from a
spike raster
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>,
which is (of course) just a matrix for a random variable (spike or
no-spike) sampled over time (rows) and trials (columns). The variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
is discrete, with only two possible values: 1 for a spike, 0 for no
spike. It does not play nicely with any of the empirical formulas given
above. For more cogent results, the rows (i.e., the time axis) of the
matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
must be downsampled via binning. This binning is done automatically by
the function <a href="../reference/load.rasters.as.neurons.html">load.rasters.as.neurons</a>
and can be controlled via its argument <strong>bin_size</strong>, which
below is represented by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œî</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math>.
The default, used in the code above, is 20ms. A further question to
decide when computing empirical autocorrelation is how to handle
multiple spikes in a single bin. There are three options supported by
the neurons package: ‚Äúsum‚Äù, ‚Äúmean‚Äù, ‚Äúboolean‚Äù. The option is set via the
argument <strong>bin_count_action</strong>, available in the functions
<a href="../reference/compute.autocorr.html">compute.autocorr</a>, <a href="../reference/process.autocorr.html">process.autocorr</a>, and <a href="../reference/estimate.autocorr.params.html">estimate.autocorr.params</a>.
In all cases, the default is ‚Äúsum‚Äù, meaning that the value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
in a given bin on a given trial is the total count of spikes falling in
that bin. The option ‚Äúmean‚Äù will instead return the mean number of
spikes in the bin, while ‚Äúboolean‚Äù will return 1 if there is at least
one spike in the bin and 0 otherwise.</p>
<p>The function <a href="../reference/compute.autocorr.html">compute.autocorr</a> takes a
single neuron and computes its empirical autocorrelation using its spike
raster. Here, for example, is the function used to compute the raw
autocorrelation for the neuron with high autocorrelation:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/compute.autocorr.html">compute.autocorr</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span>, use_raw <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/plot-autocorrelation.html">plot.autocorrelation</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/compute_autocorrelation_high-1.png" width="700"></p>
<p>And here is the Pearson autocorrelation for the same data:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/compute.autocorr.html">compute.autocorr</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span>, use_raw <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/plot-autocorrelation.html">plot.autocorrelation</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/compute_autocorrelation_high_pearson-1.png" width="700"></p>
<p>Next, the raw autocorrelation for the neuron with low
autocorrelation:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/compute.autocorr.html">compute.autocorr</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_low</span><span class="op">]</span><span class="op">]</span>, use_raw <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/plot-autocorrelation.html">plot.autocorrelation</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_low</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/compute_autocorrelation_low-1.png" width="700"></p>
<p>And the Pearson autocorrelation for the same data:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/compute.autocorr.html">compute.autocorr</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_low</span><span class="op">]</span><span class="op">]</span>, use_raw <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/plot-autocorrelation.html">plot.autocorrelation</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_low</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/compute_autocorrelation_low_pearson-1.png" width="700"></p>
<h2>
Modeling autocorrelation decay
</h2>
<p>Theoretically, autocorrelation can be expected to exhibit exponential
decay over increasing lag, at least in cases with nonzero
autocorrelation. As noted above, this decay can be modelled with the
function:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>=</mo><mi>A</mi><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>‚àí</mi><mi>l</mi><mi>/</mi><mi>œÑ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">R = A\exp(-l/\tau) + b</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
is the <em>amplitude</em> (autocorrelation at the initial lag),
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
is lag,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÑ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>
is the <em>network time constant</em>, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>
is a constant (bias or baseline) term. The neurons package assumes that
the bias term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>
is a constant function of the firing rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
and bin size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œî</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math>,
given as:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œª</mi><mrow></mrow><mi>Œî</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">b = (\lambda{}\Delta)^2</annotation></semantics></math>
In this formula both
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œî</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math>
must be in the same unit of time, e.g., ms. The values for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÑ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>
are set by minimizing the mean squared error between the empirical
autocorrelation and the model function, using the [L-BFGS algorithm as
implemented in NLopt]{<a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#low-storage-bfgs" class="external-link uri">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#low-storage-bfgs</a>}.</p>
<p>Model fitting is accessed in the neurons package with the function <a href="../reference/fit.edf.autocorr.html">fit.edf.autocorr</a>. The
function takes a single neuron and fits the exponential decay function
to its empirical autocorrelation. Here, for example, is the function
used to fit the model to the raw autocorrelation for the neuron with
high autocorrelation:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/compute.autocorr.html">compute.autocorr</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/fit.edf.autocorr.html">fit.edf.autocorr</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/plot-autocorrelation.html">plot.autocorrelation</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/fit_autocorrelation_high-1.png" width="700"></p>
<p>The fitted model is plotted as the red line. The parameters of the
exponential decay fit can be fetched directly with a neuron method and
provide succinct quantification of the empirical autocorrelation.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">[[</span><span class="va">cell_high</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="fu">fetch_EDF_parameters</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##           A         tau   bias_term </span></span>
<span><span class="co">##  0.25888154 67.42729942  0.04423672</span></span></code></pre>
<p>In this case, the time constant tau is estimated to be 67ms and the
initial autocorrelation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
is estimated to be 0.26.</p>
<p>The above shows empirical autocorrelation and model fit being
performed in separate steps and for only one neuron at a time. The
function <a href="../reference/process.autocorr.html">process.autocorr</a> will
perform both steps at once for an entire list of neurons and return the
results in a data frame.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">autocor.results.batch</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/process.autocorr.html">process.autocorr</a></span><span class="op">(</span><span class="va">neurons</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">autocor.results.batch</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##       cell    lambda_ms  lambda_bin            A        tau    bias_term   autocorr1 max_autocorr mean_autocorr min_autocorr</span></span>
<span><span class="co">## 1 neuron_1 0.0021196442 0.042392883  0.077501060   1.666924 1.797157e-03 0.079298216 0.0017976336  1.797163e-03 1.797157e-03</span></span>
<span><span class="co">## 2 neuron_2 0.0003823936 0.007647872  0.001298119  19.999968 5.848995e-05 0.001356608 0.0005360403  6.856292e-05 5.848995e-05</span></span>
<span><span class="co">## 3 neuron_3 0.0036099494 0.072198987  0.017915334 170.115506 5.212694e-03 0.023128028 0.0211408733  7.101940e-03 5.215053e-03</span></span>
<span><span class="co">## 4 neuron_4 0.0017867616 0.035735233  0.003252084 856.788320 1.277007e-03 0.004529091 0.0044540564  2.781459e-03 1.828696e-03</span></span>
<span><span class="co">## 5 neuron_5 0.0096557914 0.193115827 -0.022580725  44.047935 3.729372e-02 0.014712998 0.0372937227  3.677671e-02 2.295382e-02</span></span>
<span><span class="co">## 6 neuron_6 0.0087146980 0.174293960  0.279085447   2.336045 3.037838e-02 0.309463832 0.0304317837  3.037909e-02 3.037838e-02</span></span></code></pre>
<h2>
Estimating network time constant
</h2>
<p>The above discussion concerns computing and modeling
<em>empirical</em> autocorrelation, i.e., autocorrelation as computed
directly off a finite sample. However, what‚Äôs usually desired is an
estimate of the <em>population</em> value, i.e., the true
autocorrelation exhibited by a population of cells defined by some
shared covariate value. Estimating this true value is done by taking an
infinite sample, sampling not just all existing population members, but
also all possible members. This is, of course, impossible. However, it
can be approximated by taking larger and larger samples. The ideal, of
course, would be to take samples of the actual population, e.g.,
recording more cells, or recording more trials from the same cells.
However, in practice this is not possible. Instead, mathematical
techniques are used to simulate larger samples from existing data. The
most well-known technique is bootstrapping, i.e., ‚Äúresampling‚Äù the
observed data with replacement. Bootstrapping does a good job perserving
the underlying statistical distribution of data when the signal-to-noise
ratio is high, but when the signal-to-noise ratio is low, bootstrapping
will simply amplify the noise. This is the case with autocorrelation
estimated from spike rasters, especially when the firing rate is low and
the recording time is short.</p>
<h3>
Dichotomized Gaussians
</h3>
<p>Instead of resampling with replacement, an alternative approach is to
simulate new samples through a random process model that‚Äôs constrained
to be consistent with the observed data. In the case of exponentially
decaying autocorrelation, dichotomized Gaussians provide an ideal model.
The basic idea is to model the noisy processes underlying neuron spiking
across time as a latent multivariate Gaussian process, with one Gaussian
distribution per time bin. On this model, autocorrelation is modelled as
correlation between these Gaussians.</p>
<p>To get a better handle on the idea, consider the following example.
First, let‚Äôs draw a random sample of 300 points from a bivariate
Gaussian distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math>,
such that both component distributions
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mn>1</mn></msub><annotation encoding="application/x-tex">V_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mn>2</mn></msub><annotation encoding="application/x-tex">V_2</annotation></semantics></math>
are normal (i.e., have mean
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œº</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math>
of 0 and standard deviation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÉ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>
of 1) and a covariance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mi>V</mi></msub><annotation encoding="application/x-tex">K_V</annotation></semantics></math>
of 0.75 exists between these distributions.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">V_sample</span> <span class="op">&lt;-</span> <span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/mvrnorm.html" class="external-link">mvrnorm</a></span><span class="op">(</span></span>
<span>    n <span class="op">=</span> <span class="fl">300</span>,</span>
<span>    mu <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span><span class="op">)</span>,</span>
<span>    Sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span> </span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0.75</span>, </span>
<span>          <span class="fl">0.75</span>, <span class="fl">1</span><span class="op">)</span>, </span>
<span>        nrow <span class="op">=</span> <span class="fl">2</span>, </span>
<span>        ncol <span class="op">=</span> <span class="fl">2</span></span>
<span>      <span class="op">)</span> </span>
<span>  <span class="op">)</span></span></code></pre></div>
<p>Next, let‚Äôs plot these points and superimpose on top of them
thresholds
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ≥</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma = 1</annotation></semantics></math>
for both dimensions, shading the area of points below the threshold.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Convert to data frame for plotting</span></span>
<span><span class="va">V_sample</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">V_sample</span><span class="op">)</span> </span>
<span><span class="va">threshold</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="co"># Make and print plot</span></span>
<span><span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">V_sample</span>, <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">V_sample</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, y <span class="op">=</span> <span class="va">V_sample</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">labs</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="st">"V1"</span>, </span>
<span>    y <span class="op">=</span> <span class="st">"V2"</span>, </span>
<span>    title <span class="op">=</span> <span class="st">"Example bivariate data"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html" class="external-link">ylim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">4.5</span>,<span class="fl">4.5</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html" class="external-link">xlim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">4.5</span>,<span class="fl">4.5</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html" class="external-link">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html" class="external-link">theme</a></span><span class="op">(</span></span>
<span>    panel.background <span class="op">=</span> <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html" class="external-link">element_rect</a></span><span class="op">(</span>fill <span class="op">=</span> <span class="st">"white"</span>, colour <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span>,</span>
<span>    plot.background  <span class="op">=</span> <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html" class="external-link">element_rect</a></span><span class="op">(</span>fill <span class="op">=</span> <span class="st">"white"</span>, colour <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html" class="external-link">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">0</span>, color <span class="op">=</span> <span class="st">"darkgray"</span>, linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html" class="external-link">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span>, color <span class="op">=</span> <span class="st">"darkgray"</span>, linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html" class="external-link">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="va">threshold</span>, color <span class="op">=</span> <span class="st">"darkblue"</span>, linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html" class="external-link">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="va">threshold</span>, color <span class="op">=</span> <span class="st">"darkblue"</span>, linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html" class="external-link">annotate</a></span><span class="op">(</span></span>
<span>    <span class="st">"rect"</span>, </span>
<span>    xmin <span class="op">=</span> <span class="op">-</span><span class="cn">Inf</span>, xmax <span class="op">=</span> <span class="va">threshold</span>, </span>
<span>    ymin <span class="op">=</span> <span class="op">-</span><span class="cn">Inf</span>, ymax <span class="op">=</span> <span class="va">threshold</span>, </span>
<span>    fill <span class="op">=</span> <span class="st">"lightblue"</span>, alpha <span class="op">=</span> <span class="fl">0.3</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html" class="external-link">annotate</a></span><span class="op">(</span></span>
<span>    <span class="st">"text"</span>, x <span class="op">=</span> <span class="va">threshold</span> <span class="op">+</span> <span class="fl">0.35</span>, y <span class="op">=</span> <span class="fl">4.5</span>, </span>
<span>    label <span class="op">=</span> <span class="st">"gamma"</span>, parse <span class="op">=</span> <span class="cn">TRUE</span>, color <span class="op">=</span> <span class="st">"darkblue"</span>, </span>
<span>    size <span class="op">=</span> <span class="fl">7</span>, hjust <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html" class="external-link">annotate</a></span><span class="op">(</span></span>
<span>    <span class="st">"text"</span>, x <span class="op">=</span> <span class="fl">4.5</span>, y <span class="op">=</span> <span class="va">threshold</span> <span class="op">+</span> <span class="fl">0.35</span>, </span>
<span>    label <span class="op">=</span> <span class="st">"gamma"</span>, parse <span class="op">=</span> <span class="cn">TRUE</span>, color <span class="op">=</span> <span class="st">"darkblue"</span>, </span>
<span>    size <span class="op">=</span> <span class="fl">7</span>, vjust <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<p><img src="tutorial_tau_est_DG_files/figure-html/DG_plot-1.png" width="700"></p>
<h3>
Simulating spike rate
</h3>
<p>Suppose a threshold value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≥</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>
defines, for any
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mi>i</mi></msub><annotation encoding="application/x-tex">V_i</annotation></semantics></math>,
a new binary random variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
such that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">X=1</annotation></semantics></math>
if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>i</mi></msub><mo>&gt;</mo><mi>Œ≥</mi></mrow><annotation encoding="application/x-tex">V_i&gt;\gamma</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">X=0</annotation></semantics></math>
otherwise. In this case, the probability
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>=</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(X=1)</annotation></semantics></math>
that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
is 1 is given by the cumulative distribution function of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mi>i</mi></msub><annotation encoding="application/x-tex">V_i</annotation></semantics></math>
evaluated at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≥</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>.
As each
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mi>i</mi></msub><annotation encoding="application/x-tex">V_i</annotation></semantics></math>
is stipulated to be a standard normal
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œº</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu=0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÉ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sigma=1</annotation></semantics></math>),
this cumulative distribution function is the standard normal cumulative
distribution function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ¶</mi><annotation encoding="application/x-tex">\Phi</annotation></semantics></math>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ¶</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><msqrt><mrow><mn>2</mn><mi>œÄ</mi></mrow></msqrt></mfrac><msubsup><mo>‚à´</mo><mrow><mi>‚àí</mi><mi>‚àû</mi></mrow><mi>x</mi></msubsup><msup><mi>e</mi><mrow><mi>‚àí</mi><msup><mi>t</mi><mn>2</mn></msup><mi>/</mi><mn>2</mn></mrow></msup><mi>d</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">\Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-t^2/2} dt</annotation></semantics></math>
Thus:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>=</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>V</mi><mi>i</mi></msub><mo>&gt;</mo><mi>Œ≥</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>‚àí</mo><mi>Œ¶</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≥</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(X=1) = P(V_i&gt;\gamma) = 1 - \Phi(\gamma)</annotation></semantics></math>
So, for example,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mi>i</mi></msub><annotation encoding="application/x-tex">V_i</annotation></semantics></math>
can be used to simulate a neuron with mean spike rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
by setting the threshold
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≥</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>
such that:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œª</mi><mo>=</mo><mn>1</mn><mo>‚àí</mo><mi>Œ¶</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≥</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\lambda = 1 - \Phi(\gamma)</annotation></semantics></math>
which means that:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ≥</mi><mo>=</mo><msup><mi>Œ¶</mi><mrow><mi>‚àí</mi><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>Œª</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\gamma = \Phi^{-1}(1-\lambda)</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Œ¶</mi><mrow><mi>‚àí</mi><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\Phi^{-1}</annotation></semantics></math>
is the inverse of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ¶</mi><annotation encoding="application/x-tex">\Phi</annotation></semantics></math>,
i.e., is the quantile function. This quantile function can be computed
via well-known numerical approximations, meaning that a dichotomized
Gaussian can easily be used to simulate a neuron with a desired mean
spike rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>.</p>
<h3>
Simulating autocorrelation
</h3>
<p>While simulating a given spike rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
is straightforward, simulating autocorrelation is not. How is
autocorrelation to be represented in the model? If the correlation
between two component dimensions
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><annotation encoding="application/x-tex">V_{i_1}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub><annotation encoding="application/x-tex">V_{i_2}</annotation></semantics></math>
of a multivariate Gaussian are to represent the autocorrelation between
two time bins
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><annotation encoding="application/x-tex">X_{i_1}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub><annotation encoding="application/x-tex">X_{i_2}</annotation></semantics></math>
of a spike raster separated by lag
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>,
then each
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mi>i</mi></msub><annotation encoding="application/x-tex">V_i</annotation></semantics></math>
should not be thought of as a separate neuron, but rather as the same
neuron at different time points.</p>
<p>While this is an insightful idea, it‚Äôs mathematically impossible. The
issue is that the operation of thresholding, needed to convert a
Gaussian variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><mi>i</mi></msub><annotation encoding="application/x-tex">V_i</annotation></semantics></math>
into a simulated binary spike variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding="application/x-tex">X_i</annotation></semantics></math>,
will change the correlation:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo>‚â†</mo><msub><mi>R</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub></mrow><annotation encoding="application/x-tex">R_{V_{i_1}V_{i_2}} \neq R_{X_{i_1}X_{i_2}}</annotation></semantics></math>
However, just as there is a predictable relationship between the
threshold
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≥</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>
and spike rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>,
there is also a predictable relationship between the correlations
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">R_{V_{i_1}V_{i_2}}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">R_{X_{i_1}X_{i_2}}</annotation></semantics></math>.</p>
<p>Let‚Äôs derive this relationship. Suppose we know the autocorrelation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">R_{X_{i_1}X_{i_2}}</annotation></semantics></math>
between two time bins
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><annotation encoding="application/x-tex">X_{i_1}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub><annotation encoding="application/x-tex">X_{i_2}</annotation></semantics></math>
separated by lag
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>
and know that the neuron
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
spikes with rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
across all time bins. We want to find the correlation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">R_{V_{i_1}V_{i_2}}</annotation></semantics></math>
between the corresponding Gaussian variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><msub><mi>i</mi><mn>1</mn></msub></msub><annotation encoding="application/x-tex">V_{i_1}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>V</mi><msub><mi>i</mi><mn>2</mn></msub></msub><annotation encoding="application/x-tex">V_{i_2}</annotation></semantics></math>
which, when thresholded by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ≥</mi><mo>=</mo><msup><mi>Œ¶</mi><mrow><mi>‚àí</mi><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>Œª</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\gamma = \Phi^{-1}(1-\lambda)</annotation></semantics></math>,
gives back the correlation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">R_{X_{i_1}X_{i_2}}</annotation></semantics></math>.</p>
<p>Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ¶</mi><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>K</mi><mi>V</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Phi_2(x,K_V)</annotation></semantics></math>
be the cumulative distribution function of a multivariate normal
distribution
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œº</mi><msub><mi>V</mi><mi>i</mi></msub></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu_{V_i}=0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÉ</mi><msub><mi>V</mi><mi>i</mi></msub></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sigma_{V_i}=1</annotation></semantics></math>
for all components
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>)
with covariance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>,
evaluated at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
for all components:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ¶</mi><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>K</mi><mi>V</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>V</mi><mn>1</mn></msub><mo>‚â§</mo><mi>x</mi><mo>,</mo><msub><mi>V</mi><mn>2</mn></msub><mo>‚â§</mo><mi>x</mi><mo>,</mo><mi>‚Ä¶</mi><mspace width="0.278em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.278em"></mspace><msub><mi>V</mi><mn>1</mn></msub><mo>,</mo><msub><mi>V</mi><mn>2</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>‚àº</mo><mtext mathvariant="normal">MVN</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œº</mi><mo>=</mo><mn>0</mn><mo>,</mo><mi>œÉ</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>K</mi><mo>=</mo><msub><mi>K</mi><mi>V</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Phi_2(x,K_V) = P(V_1\leq x, V_2\leq x, \ldots \;|\; V_1, V_2, \ldots \sim \text{MVN}(\mu=0,\sigma=1,K=K_V))</annotation></semantics></math>
That is,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ¶</mi><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>K</mi><mi>V</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Phi_2(x,K_V)</annotation></semantics></math>
is the probability that all components of a multivariate normal
distribution with covariance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mi>V</mi></msub><annotation encoding="application/x-tex">K_V</annotation></semantics></math>
are less than or equal to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>.</p>
<p>By definition, the empirical raw autocorrelation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><annotation encoding="application/x-tex">R_{X_{i_1}X_{i_2}}</annotation></semantics></math>
can be written in terms of autocovariance, represented by the variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>,
as follows:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo>=</mo><msub><mi>K</mi><mrow><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></mrow></msub><mo>+</mo><msub><mi>Œº</mi><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub></msub><msub><mi>Œº</mi><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub></msub><mo>=</mo><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><msub><mi>i</mi><mn>1</mn></msub></msub><mo>,</mo><msub><mi>X</mi><msub><mi>i</mi><mn>2</mn></msub></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msup><mi>Œª</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R_{X_{i_1}X_{i_2}} = K_{X_{i_1}X_{i_2}} + \mu_{X_{i_1}}\mu_{X_{i_2}} = \text{Cov}(X_{i_1}, X_{i_2}) + \lambda^2</annotation></semantics></math></p>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Michael Barkasi.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
